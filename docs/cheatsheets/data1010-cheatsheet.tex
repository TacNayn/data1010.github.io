\documentclass[prettycode]{worksheet}
\usepackage{multicol, lipsum}
\usepackage[fontsize=6pt]{scrextend} 
\hypersetup{colorlinks=false} 
\geometry{landscape,top=0.25in}

\usepackage{eso-pic, rotating}

\AddToShipoutPicture{\put(10,215){\rotatebox{90}{%
      \scalebox{1.2}{\tcbox[boxrule = 0.2pt,
        top=0mm,bottom=0mm,left=0mm,right=0mm]{\fontspec{Augie}
          \color{gray} DATA 1010 $\cdot$ Brown
        University $\cdot$ Samuel S.\ Watson}}}}}

\setmainfont[BoldFont=texgyrepagella-bold.otf,
             ItalicFont     = texgyrepagella-italic.otf ,
             BoldItalicFont = texgyrepagella-bolditalic.otf ]{TeX Gyre Pagella} 

\DeclareRobustCommand{\myenumbox}[4]{\tikz[baseline=-0.65ex]
  \node[
  minimum width = 7pt, 
  inner sep=1.3pt,
  draw=#1,
  fill=#2,
  text=#3,
  rounded corners=1pt]{\small \bf \fontspec{American Typewriter Bold} #4};}

 \NewTCBListing{julblock}{!O{}}{
      minted language = julia,
      listing only,
      colback = softred,
      colframe = softred,
      top = 0pt,
      bottom = 0pt, 
      before skip = 0pt,
      after skip = 0pt, 
      minted options={autogobble},
      #1
    }


    \NewTCBListing{Rblock}{!O{}}{
      minted language = R,
      listing only,
      colback = softyellow,
      colframe = softyellow, 
      top = 0pt,
      bottom = 0pt, 
      before skip = 0pt,
      after skip = 0pt, 
      minted options={autogobble},
      #1
    }

\newcommand{\jul}[1]{\mintinline{julia}{#1}}

\setlist[enumerate]{label=(\roman*),itemsep=6pt} 
\setlist[enumerate,1]{leftmargin=12pt}
\setlist[enumerate,2]{label=\arabic*.}
\setlist[enumerate,3]{label=(\arabic*)}

\newcounter{boxcounter}
\setcounter{boxcounter}{1}

\newcommand{\pbox}{\stepcounter{boxcounter}\myenumbox{MidnightBlue}{Purple!75!black}{Purple!5!white}{\theboxcounter}\hspace{2.5pt}}
\renewcommand{\greenbox}{\stepcounter{boxcounter}\myenumbox{MidnightBlue}{SeaGreen}{softseagreen}{\theboxcounter}\hspace{2.5pt}}
\renewcommand{\redbox}{\stepcounter{boxcounter}\myenumbox{MidnightBlue}{DarkRed!75!white}{softred}{\theboxcounter}\hspace{2.5pt}}
\renewcommand{\bluebox}{\stepcounter{boxcounter}\myenumbox{MidnightBlue}{MidnightBlue!75!white}{softblue}{\theboxcounter}\hspace{2.5pt}}
\newcommand{\ybox}{\stepcounter{boxcounter}\myenumbox{MidnightBlue}{yellow!40!black}{softyellow}{\theboxcounter}\hspace{2.5pt}}

\begin{document}

\begin{multicols*}{4}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Sets and Functions: Sets},
    colframe = MidnightBlue!75!white, breakable, left = 2mm, right = 2mm, 
    colback = softblue, boxsep = 0.5pt, before upper = \setcounter{boxcounter}{0},
    enhanced, parbox = false, drop fuzzy shadow]

    \bluebox A \textbf{set} is an unordered collection of objects. The
    objects in a set are called \textit{elements}.
    
    \bluebox The \textbf{cardinality} of a set is the number of
    elements it contains. The \textbf{empty set} $\emptyset$ is the set with no
    elements.
    
    \bluebox If every element of $A$ is also an element of $B$, then
    we say $A$ is a \textbf{subset} of $B$ and write $A \subset B$. If
    $A \subset B$ and $B \subset A$, then we say that $A = B$. 

    \bluebox Set operations: 
    \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
    \item An element is in the \textbf{union} $A\cup B$ of two sets $A$ and
      $B$ if it is in $A$ or $B$.
    \item An element is in the \textbf{intersection} $A \cap B$ of two
      sets $A$ and $B$ if it is in $A$ and $B$.
    \item An element is in the \textbf{set difference} $A \setminus B$
      if it is in $A$ but not $B$.
    \item Given a set $\Omega$ and a set $A \subset \Omega$, the
      \textbf{complement} of $A$ with respect to $\Omega$ is
      $A\complement = \Omega \setminus A$.
    \end{enumerate}

    \begin{center}
      \includegraphics[width=0.22\textwidth]{figures/union}
      \includegraphics[width=0.22\textwidth]{figures/intersect}
      \includegraphics[width=0.22\textwidth]{figures/setdiff}
      \includegraphics[width=0.22\textwidth]{figures/complement}
    \end{center}

    \bluebox Two sets $A$ and $B$ are \textbf{disjoint} if
    $A \cap B = \emptyset$ (in other words, if they have no elements
    in common).

    \bluebox A \textbf{partition} of a set is a collection of nonempty
    disjoint subsets whose union is the whole set.

    \bluebox The \textbf{Cartesian product} of $A$ and $B$ is
    \[
      A \times B = \{(a,b) : a \in A \text{ and } b \in
      B\}. 
    \]

    \vspace{-2mm}

    \bluebox (\textbf{De Morgan's laws}) If $A,B \subset \Omega$, then 
    \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
    \item $(A \cap B) \complement = A\complement \cup B\complement$,
      and 
    \item $(A \cup B) \complement = A\complement \cap B\complement$. 
    \end{enumerate}

    \bluebox A \textbf{list} is an ordered collection of finitely many
    objects. Lists differ from sets in that (i) order matters, (ii)
    repetition matters, and (iii) the cardinality is restricted. 
  \end{tcolorbox}
  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Sets and Functions: Functions},
    colframe = MidnightBlue!75!white, breakable, left = 2mm, right = 2mm, 
    colback = softblue, boxsep = 0.5pt, before upper = \setcounter{boxcounter}{0},
    enhanced, parbox = false, drop fuzzy shadow]

    \bluebox If $A$ and $B$ are sets, then a \textbf{function}
    $f:A \to B$ is an assignment of some element of $B$ to each
    element of $A$. 

    \bluebox The set $A$ is called the \textbf{domain} of $f$ and $B$ is called
    the \textbf{codomain} of $f$.

    \bluebox Given a subset $A'$ of $A$, we define the \textbf{image}
    of $f$---denoted $f(A')$---to be the set of elements which are
    mapped to from some element in $A'$. 

    \bluebox The \textbf{range} of $f$ is the image of the domain of
    $f$.

    \bluebox The \textbf{composition} of two functions $f: A \to B$
    and $g:B \to C$ is the function $g\circ f$ which maps $a \in A$ to
    $g(f(a))\in C$.

    \bluebox The \textbf{identity function} on a set $A$ is the
    function $f:A \to A$ which maps each element to itself. 

    \bluebox A function $f$ is \textbf{injective} if no two elements in the
    domain map to the same element in the codomain. 
    
    \bluebox A function $f$ is \textbf{surjective} if the range of $f$ is equal
    to the codomain of $f$. 
    
    \bluebox A function $f$ is \textbf{bijective} if it is both injective and
    surjective. If $f$ is bijective, then the function from $B$ to $A$
    that maps $b\in B$ to the element $a \in A$ that satisfies
    $f(a) = b$ is called the \textbf{inverse} of $f$.

    \bluebox If $f:A \to B$ is bijective, then the function
    $f^{-1}\circ f$ is equal to the identity function on $A$, and
    $f\circ f^{-1}$ is the identity function on $B$.
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Programming in Julia}, 
    colframe = DarkRed!75!white, breakable, left = 2mm, right = 2mm, 
    colback = softred, boxsep = 0.5pt, before upper = \setcounter{boxcounter}{0},
    enhanced, parbox = false, drop fuzzy shadow]

    \redbox A \textbf{value} is a fundamental entity that may be manipulated by a
    program. Values have \textbf{types}; for example, \jul{5} is
    an \jul{Int} and \jul{"Hello world!"} is a
    \jul{String}.
    
    \redbox A \textbf{variable} is a name used to refer to a value. We can
    \textbf{assign} a value \jul{5} to a variable \jul{x} using \jul{x = 5}.
    
    \redbox A \textbf{function} performs a particular task. You prompt
    a function to perform its task by \textbf{calling} it. Values
    supplied to a function are called \textbf{arguments}. For example,
    in the function call \jul{print(1,2)}, \jul{1} and \jul{2} are
    arguments.
    
    \redbox An \textbf{operator} is a function that can be called in a
    special way. For example, \jul{*} is an operator since we can call
    the multiplication function with the syntax \jul{3 * 5}.

    \redbox A \textbf{statement} is an instruction to be executed
    (like \jul{x = -3}). An \textbf{expression} is a combination of values, variables, operators, and function calls that a language
    interprets and \textbf{evaluates} to a value.

    \redbox A numerical value can be either an \textbf{integer} or a
    \textbf{float}. The basic operations are \jul{+,-,*,/,^}, and
    expressions are evaluated according to the order of operations.

    \redbox Numbers can be compared using \jul{<,>,==,≤} or \jul{≥}. 

    \redbox Textual data is represented using \textbf{strings}.
    \jul{length(s)} returns the number of characters in \jul{s}. The
    \jul{*} operator concatenates strings.

    \redbox A \textbf{boolean} is a value which is either \jul{true} or
    \jul{false}. Booleans can be combined with the operators
    \mintinline{julia}{&&} (and),
    \mintinline{julia}{||} (or), 
    \mintinline{julia}{!} (not).

    \redbox Code blocks can be executed conditionally:
    \begin{julblock}
         if x > 0
             "x is positive"
         elseif x == 0
             "x is zero"
         else
             "x is negative"
         end      
    \end{julblock}

    \redbox Functions may be defined using the familiar math notation: 
    \jul{f(x,y) = 3x + 2y} or using a \jul{function} block (\jul{shift}
    is a \textbf{keyword argument}): 
    \begin{julblock}
      function f(x,y; shift=0)
          3x + 2y + shift
      end
    \end{julblock}

    \redbox The \textbf{scope} of a variable is the region in the
    program where it is accessible. Variables defined in the body of a
    function are not accessible outside the body of the function.

    \redbox \jul{Array} is a compound data type for storing lists of
    objects. Entries of an array may be accessed with square bracket
    syntax using an index or using a \textbf{range} object \jul{a:b}.
    \begin{julblock}
      A = [-5,3,2,1]
      A[2]
      A[3:end]
    \end{julblock}
    
    \redbox An \textbf{array comprehension} can be used to generate
    new arrays: \jul{[k^2 for k=1:10 if mod(k,2) == 0]}

    \redbox A \textbf{dictionary} encodes a discrete function by
    storing input-output pairs and looking up input values when
    indexed. This expression returns \jul{[0,0,1.0]}: 

    \hspace{3mm}\jul{Dict("blue"=>[0,0,1.0],"red"=>[1.0,0,0])["blue"]} 

    \redbox A \jul{while} loop takes a conditional expression and a
    body and evaluates them alternatingly until the conditional
    expression returns false. A \jul{for} loop evaluates its body once
    for each entry in a given \textit{iterator} (for example, a range,
    array, or dictionary). Each value in the iterator is
    assigned to a loop variable which can be referenced in the body of
    the loop.

    \begin{tcbraster}[raster columns = 2, top = 0mm, bottom = 0mm,
      raster before skip = 0mm, raster after skip = 0mm, left = 0mm]
      \begin{julblock}
        while x > 0
            x -= 1
        end
      \end{julblock}
      \begin{julblock}
        for i=1:10
            print(i)
        end
      \end{julblock}
    \end{tcbraster}
  \end{tcolorbox}  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Linear Algebra: Vector Spaces},
    colframe = SeaGreen, breakable, left = 2mm, right = 2mm, 
    colback = softseagreen, boxsep = 0.5pt, before upper = \setcounter{boxcounter}{0},
    enhanced, parbox = false, drop fuzzy shadow]

    \greenbox A \textbf{vector} in $\R^n$ is a column of $n$ real
    numbers, also written as $[v_1, \ldots, v_n]$. A vector may be
    depicted as an arrow from the origin in $n$-dimensional space. The
    \textbf{norm} of a vector $\mathbf{v}$ is the length
    $\sqrt{v_1^2 + \cdots + v_n^2}$ of its arrow.

    \greenbox The fundamental vector space operations are \textbf{vector addition} and
    \textbf{scalar multiplication}.

    \begin{center}
      \includegraphics[width=1.25cm]{figures/vecadd} \hspace{5mm} 
      \includegraphics[width=1.25cm]{figures/vecscale}
    \end{center}

    \greenbox A \textbf{linear combination} of a list of vectors $\mathbf{v}_1,
    \ldots, \mathbf{v}_k$ is an expression of the form 
    \[
      c_1\mathbf{v_1} + c_2\mathbf{v_2}  + \cdots +
      c_k\mathbf{v_k}, 
    \]
    where $c_1, \ldots, c_k$ are real numbers. The $c$'s are called
    the \textbf{weights} of the linear combination.
    
    \greenbox The \textbf{span} of a list $L$ of vectors is the set of
  all vectors which can be written as a linear combination of the
  vectors in $L$.

  \greenbox A list of vectors is \textbf{linearly independent} if and
  only if the only linear combination which yields the zero vector
  is the one with all weights zero.

  \greenbox A \textbf{vector space} is a nonempty set of vectors which
  is closed under the vector space operations.

  \greenbox A list of vectors in a vector space is a \textbf{spanning
    list} of that vector space if every vector in the vector space can be written as a
  linear combination of the vectors in that list.

  \greenbox A linearly independent spanning list of a vector space is
  called a \textbf{basis} of that vector space. The number of vectors
  in a basis of a vector space is called the \textbf{dimension} of the
  space.

  \greenbox A \textbf{linear transformation} $L$ is a function from a
  vector space $V$ to a vector space $W$ which satisfies
  $L(c \mathbf{v} + \beta \mathbf{w}) = c L(\mathbf{v}) +
  L(\mathbf{w})$ for all $c \in \R$, $\mathbf{u}, \mathbf{v} \in
  V$. These are ``flat maps'': equally spaced lines are mapped to
  equally spaces lines or points. Examples: scaling, rotation,
  projection, reflection.

  \greenbox Given two vector spaces $V$ and $W$, a basis
  $\{\mathbf{v}_1, \ldots, \mathbf{v}_n\}$ of $V$, and a list
  $\{\mathbf{w}_1, \ldots, \mathbf{w}_n\}$ of vectors in $W$, there
  exists one and only one linear transformation which maps
  $\mathbf{v}_1$ to $\mathbf{w}_1$, $\mathbf{v}_2$ to $\mathbf{w}_2$,
  and so on.
  
  \greenbox The \textbf{rank} of a linear transformation from one vector
  space to another is the dimension of its range.
  
  \greenbox The \textbf{null space} of a linear transformation is the
  set of vectors which are mapped to the zero vector by the linear
  transformation.
  
  \greenbox The rank of a transformation plus the dimension of its
  null space is equal to the dimension of its domain (the
  \textbf{rank-nullity theorem}). 
  
\end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Linear Algebra: Matrix Algebra},
    colframe = SeaGreen, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softseagreen, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]
    
    \greenbox The \textbf{matrix-vector product} $A\mathbf{x}$ is the
    linear combination of the columns of $A$ with weights given by the
    entries of $\mathbf{x}$.
    
    \greenbox Linear transformations from $\R^n$ to $\R^m$ are in
    one-to-one correspondence with $m\times n$ matrices.

    \greenbox The identity transformation corresponds to the
    \textbf{identity matrix}, which has entries of 1 along the
    diagonal and zero entries elsewhere.

    \greenbox \textbf{Matrix multiplication} corresponds to
    composition of the corresponding linear transformations: $AB$ is
    the matrix for which $(AB)(\mathbf{x}) = A(B\mathbf{x})$ for all
    $\mathbf{x}$.

    \greenbox A $m\times n$ matrix is \textbf{full rank} if its rank
    is equal to $\min(m,n)$

    \greenbox $A \mathbf{x} = \mathbf{b}$ has a solution $\mathbf{x}$
    if and only if $\mathbf{b}$ is in the span of the columns of
    $A$. If $A \mathbf{x} = \mathbf{b}$ does have a solution, then the
    solution is unique if and only if the columns of $A$ are linearly
    independent. If $A \mathbf{x} = \mathbf{b}$ does not have a
    solution, then there is a unique vector $\mathbf{x}$ which
    minimizes $|A\mathbf{x} - \mathbf{b}|^2$. 

    \greenbox If the columns of a square matrix $A$ are linearly
    independent, then it has a unique \textbf{inverse matrix}
    $A^{-1}$ with the property that $A\mathbf{x} = \mathbf{b}$
    implies $\mathbf{x} = A^{-1}\mathbf{b}$ for all $\mathbf{x}$ and
    $\mathbf{b}$. 

    \greenbox Matrix inversion satisfies $(AB)^{-1} = B^{-1} A^{-1}$
    if $A$ and $B$ are both invertible.

    \greenbox The \textbf{transpose} $A\transpose$ of a matrix $A$ is
    defined so that the rows of $A\transpose$ are the columns of $A$
    (and vice versa).

    \greenbox The transpose is a linear operator:
    $(cA+B)\transpose = cA\transpose + B\transpose$ if $c$ is a
    constant and $A$ and $B$ are matrices.

    \greenbox The transpose distributes across matrix multiplication
    but with an order reversal:
    $(AB)\transpose = B\transpose A\transpose$ if $A$ and $B$ are
    matrices for which $AB$ is defined.

    \greenbox A matrix $A$ is \textbf{symmetric} if $A =
    A\transpose$.

    \greenbox A linear transformation $T$ from $\R^n$ to $\R^n$ scales all
    $n$-dimensional volumes by the same factor: the (absolute value
    of the) \textbf{determinant} of $T$.

    \greenbox The sign of the determinant tells us whether $T$ reverses
    orientations. 

    \greenbox $\det AB = \det A \det B$ and $\det A^{-1} = (\det
    A)^{-1}$. 

    \greenbox A square matrix is invertible if and only if its
    determinant is nonzero. 
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Linear Algebra: Orthogonality},
    colframe = SeaGreen, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softseagreen, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]
    
    \greenbox The \textbf{dot product} of two vectors in $\R^n$ is defined by
    \[
      \mathbf{x} \cdot \mathbf{y} = x_1y_1 + x_2y_2 + \cdots + x_n
      y_n. 
    \]
    \greenbox
    $\mathbf{x} \cdot \mathbf{y} = \|\mathbf{x}\|
    \|\mathbf{y}\|\cos\theta$, where $\mathbf{x},\mathbf{y} \in \R^n$
    and $\theta$ is the angle between the vectors. 
    
    \greenbox $\mathbf{x} \cdot \mathbf{y}  = 0$ if and only if $\mathbf{x}$
    and $\mathbf{y}$ are orthogonal. 
    
    \greenbox The dot product is linear: $\mathbf{x} \cdot (c\mathbf{y} +
    \mathbf{z})  = c \mathbf{x} \cdot \mathbf{y} +
    \mathbf{x}  \cdot \mathbf{z}$. 
    
    \greenbox The \textbf{orthogonal complement} of a subspace
    $V \subset \R^n$ is the set of vectors which are orthogonal to
    every vector in $V$.

    \greenbox The orthogonal complement of the span of the columns of
    a matrix $A$ is equal to the null space of $A\transpose$.

    \greenbox $\operatorname{rank} A = \operatorname{rank} A'A$ for
    any matrix $A$. 

    \greenbox A list of vectors satisfying $\mathbf{v}_i \cdot
    \mathbf{v}_j  = 0$ for $i \neq j$ is \textbf{orthogonal}. An
    orthogonal list of unit vectors is \textbf{orthonormal}.

    \greenbox Every orthogonal list is linearly independent

    \greenbox A matrix $U$ has orthonormal columns if and only if
    $U\transpose U = I$. A square matrix with orthonormal columns is
    called \textbf{orthogonal}. An orthogonal matrix and its transpose
    are inverses. 

    \greenbox Orthogonal matrices represent \textbf{rigid
      transformations} (ones which preserve lengths and angles). 

    \greenbox If $U$ has orthonormal columns, then $UU\transpose$ is
    the matrix which represents projection onto the span of the
    columns of $U$.
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Linear Algebra: Spectral Analysis},
    colframe = SeaGreen, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softseagreen, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]
    
    \greenbox An \textbf{eigenvector} $\mathbf{v}$ of an $n\times n$
    matrix $A$ is a nonzero vector with the property that
    $A\mathbf{v} = \lambda \mathbf{v}$ for some $\lambda \in \R$. We
    call $\lambda$ an \textbf{eigenvalue}.

    If $\mathbf{v}$ is an eigenvector of $A$, then $A$ maps the line
    $\mathrm{span}(\{\mathbf{v}\})$ to itself:
    
    \begin{center}
      \includegraphics[width=3.25cm]{figures/gridlines}
    \end{center}

    \greenbox Eigenvectors of $A$ with distinct eigenvalues are
    linearly independent.

    \greenbox Not every $n \times n$ matrix $A$ has $n$ linearly
    independent eigenvectors. If $A$ does have $n$ linearly
    independent eigenvectors, we can make a matrix $V$ with these
    eigenvectors as columns and get 
    \[
      AV  = V \Lambda \implies A = V \Lambda V^{-1} \quad 
      \text{(\textbf{diagonalization} of $A$)}
    \]
    where $\Lambda$ is a diagonal matrix of eigenvalues.

    \greenbox If $A = V \Lambda V^{-1}$, then $A^n = V \Lambda^n
    V^{-1}$. 

    \greenbox If $A$ is a symmetric matrix, then $A$ is \textbf{orthogonally
    diagonalizable}: 
    \[
      A = V \Lambda V\transpose, 
    \]
    where $V$ is an orthogonal matrix (the \textbf{spectral
      theorem}). 

    \greenbox A symmetric matrix is \textbf{positive semidefinite} if
    its eigenvalues are all nonnegative. We define the square root of
    a positive semidefinite matrix $A  = V \Lambda V\transpose$ to be
    $V \sqrt{\Lambda} V\transpose$, where $\sqrt{\Lambda}$ is obtained
    by applying the square root function elementwise. 

  \end{tcolorbox}

    \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Linear Algebra: SVD},
    colframe = SeaGreen, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softseagreen, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]
  
    \greenbox The \textbf{Gram matrix} $A\transpose A$ of any
    $m\times n$ matrix $A$ is positive semidefinite. Furthermore,
    $|\sqrt{A\transpose A} \mathbf{x}| = |A \mathbf{x}|$ for all $x
    \in\R^n$.

    \greenbox The \textbf{singular value decomposition} is the
    factorization of any rectangular $m\times n$ matrix $A$ as $U
    \Sigma V\transpose$, where $U$ and $V$ are orthogonal and $\Sigma$
    is an $m\times n$ diagonal matrix (with diagonal entries in
    decreasing order). 

    \begin{center}
      \includegraphics[width=5cm]{figures/svd}
    \end{center}

    \greenbox The diagonal entries of $\Sigma$ are the
    \textbf{singular values of $A$}, and the columns of $U$ and $V$
    are called \textbf{left singular vectors} and \textbf{right
      singular vectors}, respectively. $A$ maps each right singular
    vector $\mathbf{v}_i$ to the corresponding left singular vector
    $\mathbf{u}_i$ scaled by $\sigma_i$. 

    \greenbox The vectors in $\R^n$ stretched the most by $A$ are the
    ones which run in the direction of the column or columns of $V$
    corresponding to the greatest singular value. Same for least. 

    \greenbox For $k \geq 1$, the $k$-dimensional vector space with
    minimal sum of squared distances to the columns of $A$
    (interpreted as points in $\R^m$) is the span of the first $k$
    columns of $U$.

    \greenbox The absolute value of the determinant of a square matrix
    is equal to the product of its singular values.
    
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Multivariable calculus},
    colframe = yellow!40!black, breakable, left = 2mm, right = 2mm, 
    colback = softyellow, boxsep = 0.5pt, before upper = \setcounter{boxcounter}{0},
    enhanced, parbox = false, drop fuzzy shadow]

    \ybox A \textbf{sequence} of real numbers
    $(x_n)_{n=1}^\infty = x_1, x_2, \ldots$ \textbf{converges} to a number
    $x \in \R$ if the distance from $x_n$ to $x$ on the number line
    can be made as small as desired by choosing $n$ sufficiently
    large. We say $\lim_{n\to\infty} x_n = x$ or $x_n \to x$.

   \ybox (\textbf{Squeeze theorem}) If $a_n \leq b_n \leq c_n$ for all $n\geq 1$ and if
   $\lim_{n\to\infty} a_n = \lim_{n\to\infty} c_n = b$, then $b_n \to
   b$ as $n\to\infty$.

   \ybox (\textbf{Comparison test}) If $\sum_{n=1}^\infty b_n$
   converges and if $|a_n| \leq b_n$ for all $n$, then
   $\sum_{n=1}^\infty a_n$ converges.

   Conversely, if $\sum_{n=1}^\infty b_n$ does not converge and $0 \leq b_n <
  a_n$, then $\Sigma_{n=1}^\infty a_n$ also does not converge. 

   \ybox  The series $\sum_{n=1}^\infty n^p$ converges if and only if
    $p < -1$.
    The series $\sum_{n=1}^\infty a^n$ converges if and only if
    $-1 < a < 1$. 

   \ybox The \textbf{Taylor series}, centered at $c$, of an infinitely differentiable
    function $f$ is defined to be
    \[
      f(c) + f'(c)(x-c) + \frac{f''(c)}{2!}(x-c)^2 +
      \frac{f'''(c)}{3!}(x-c)^3 + \cdots 
    \]

    \vspace{-1mm} 
    
    \ybox We can multiply or add Taylor series term-by-term, we can
    integrate or differentiate a Taylor series term-by-term, we can
    substitute one Taylor series into another to obtain a Taylor
    series for the composition.
    
    \ybox The \textbf{partial derivative}
    $\frac{\partial f}{\partial x}(x_0,y_0)$ of a function $f(x,y)$ at
    a point $(x_0,y_0)$ is the slope of the graph of $f$ in the
    $x$-direction at the point $(x_0,y_0)$. 

    \ybox Given $\mathbf{f}: \R^n \to \R^m$, we define
    $\partial \mathbf{f}/\partial \mathbf{x}$ to be the matrix whose
    $(i,j)$th entry is $\partial f_i/\partial x_j$. Then

    \vspace{-3mm} 
    
    \begin{align*}
      (\mathrm{i}) \: \frac{\partial}{\partial \mathbf{x}} (A \mathbf{x}) = A
      &\qquad (\mathrm{ii}) \: 
        \frac{\partial}{\partial \mathbf{x}} (\mathbf{x}\transpose A) = A\transpose
      \\
      (\mathrm{iii}) \: \frac{\partial}{\partial \mathbf{x}} (\mathbf{u}\transpose \mathbf{v})
      &= \mathbf{u}\transpose\frac{\partial \mathbf{v}}{\partial
        \mathbf{x}} + \mathbf{v}\transpose\frac{\partial
        \mathbf{u}}{\partial \mathbf{x}}. 
    \end{align*}

    \vspace{-1.5mm} 
    
    \ybox A function of two variables is \textbf{differentiable} at a
    point if its graph looks like a plane when you zoom in
    sufficiently around the point. More generally, a function 
    $\mathbf{f}: \R^n \to \R^m$ is differentiable at $\mathbf{x}$ if
    it is well-approximated by its derivative near $\mathbf{x}$:

    \vspace{-3.5mm} 
    
        \[
          \lim_{\Delta \mathbf{x} \to \mathbf{0}} \frac{\mathbf{f}(\mathbf{x} +
            \Delta \mathbf{x}) - \left(\mathbf{f}(\mathbf{x}) +
              \frac{\partial\mathbf{f}}{\partial \mathbf{x}}(\mathbf{x})\Delta
              \mathbf{x}\right)}{|\Delta \mathbf{x}|} = 0.
        \]

    \vspace{-1mm} 
    
    \ybox The \textbf{Hessian} $\mathcal{H}$ of $f:\R^n \to \R$ is the
    matrix of its second order derivatives:
    $\mathcal{H}_{i,j}(\mathbf{x}) = \frac{\partial}{\partial
      x_i}\frac{\partial}{\partial x_j}f(\mathbf{x})$. The
    \textbf{quadratic approximation} of $f$ at the origin is
    $f(\boldsymbol{0}) + \frac{\partial f}{\partial
      \mathbf{x}}(\boldsymbol{0}) \mathbf{x} +
    \frac{1}{2}\mathbf{x}\transpose \mathcal{H}(\boldsymbol{0})
    \mathbf{x}$.

    \ybox Suppose that $f$ is a continuous function defined on a closed and
    bounded subset $D$ of $\R^n$. Then: 
    \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep=2pt]
    \item $f$ realizes an absolute maximum and absolute minimum on $D$
      (the \textbf{extreme value theorem}). 
    \item Any point where $f$ realizes an extremum is either a
      critical point---meaning that $\nabla f = 0$ or $f$ is
      non-differentiable at that point---or at a point on the
      boundary.
    \item (\textbf{Lagrange multipliers}) If $f$ realizes an extremum
      at a point on a portion of the boundary which is the level set
      of a differentiable function $g$ with non-vanishing gradient
      $\nabla g$, then either $f$ is non-differentiable at that point
      or the equation
      \[
        \nabla f = \lambda \nabla g 
      \]
      is satisfied at that point, for some $\lambda \in \R$.
    \end{enumerate}
    
  \ybox If $\mathbf{r}: \R^1 \to \R^2$ and $f: \R^2 \to \R^1$, then 
    \[
      \frac{\d}{\d t}(f \circ \mathbf{r}) = \frac{\partial f}{\partial
        \mathbf{r}}(\mathbf{r}(t)) 
      \frac{\d \mathbf{r}}{\d t}(t). \quad \text{(\textbf{chain rule})}
    \]

  \ybox Integrating a function is a way of totaling up its
    values. $\iint_D f(x,y) \, \d x \, \d y$ can be interpreted as the
    mass of an object occupying the region $D$ and having mass density
    $f(x,y)$ at each point $(x,y)$. 

    \ybox Double integration over $D$: the bounds for the outer
    integral are the smallest and largest values of $y$ for any point
    in $D$, and the bounds for the inner integral are the smallest and
    largest values of $x$ for any point in a given ``$y$ =
    \text{constant}'' slice of the region.

    \ybox Polar integration over $D$: the outer integral bounds are
    the least and greatest values of $\theta$ for a point in $D$, and
    the inner integral bounds are the least and greatest values of $r$
    for any point in $D$ along each given
    ``$\theta = \text{constant}$'' ray. The area element is
    $\d A = r \, \d r \, \d \theta$.
  \end{tcolorbox}
  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Numerical Computation: machine
      arithmetic},
    colframe = DarkRed!75!white, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softred, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]
    
    \redbox Computers store numerical values as sequences of bits.
    The \textbf{type} of a numeric value specifies how to interpret
    the underlying sequence of bits as a number.

    \redbox The \jul{Int64} type uses 64 bits to represent the
    integers from $-2^{63}$ to $2^{63}-1$. For
    $0 \leq n \leq 2^{63}-1$, we represent $n$ using its binary
    representation, and for $1 \leq n \leq 2^{63}$, we represent $-n$
    using the binary representation of $2^{64}-n$. \jul{Int64}
    arithmetic is performed modulo $2^{64}$.

    \redbox The \jul{Float64} type uses 64 bits to represent real
    numbers. We call the first bit $\sigma$, the next 11 bits
    (interpreted as a binary integer) $e \in [0,2047]$, and the final
    52 bits $f \in [0,2^{52} - 1]$. If $e \notin \{0,2047\}$, then
    the number represented by $(\sigma, e, f)$ is
    \[
      x = (-1)^\sigma 2^{e-1023}\left(1 + f\left(\frac{1}{2}\right)^{52}\right). 
    \]
    The representable numbers between consecutive powers of 2 are the
    ones obtained by 52 recursive iterations of binary
    subdivision. The value of $e$ indicates the powers of 2 that $x$
    is between, and the value of $f$ indicates the position of $x$
    between those powers of 2.

    The \jul{Float64} exponent value $e = 2047$ is reserved for
    \jul{Inf} and \jul{NaN}, while $e = 0$ is reserved for the
    \textbf{subnormal numbers}: $(\sigma,0,f)$ represents
    $(-1)^\sigma f/2^{1074}$. 

    \includegraphics[width=\textwidth]{figures/float64}

    \redbox The \jul{BigInt} and \jul{BigFloat} are types use an
    arbitrary number of bits and can handle very large numbers or very
    high precision. Computations are much slower than for 64-bit
    types.     
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Numerical Computation: Error},
    colframe = DarkRed!75!white, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softred, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow, break at = 170pt/0pt]

    \redbox If $\widehat{A}$ is an approximation for $A$, then the
    \textbf{relative error} is $\frac{\widehat{A} - A}{A}$

    \redbox \textbf{Roundoff error} comes from rounding numbers to fit them into
    a floating point representation.

    \redbox \textbf{Truncation error} comes from using approximate
    mathematical formulas or algorithms.

    \redbox \textbf{Statistical error} arises from using randomness in
    an approximation.

    \redbox The \textbf{condition number} of a function measures how
    it stretches or compresses \textit{relative} error. The condition
    number of a problem is the condition number of the map from the
    problem's initial data $a$ to its solution $\mathbf{S}(a)$: 
    \[
      \kappa(a) = \frac{|a||\frac{\d}{\d a}\mathbf{S}(a)|}{|\mathbf{S}(a)|}.
    \]

    \redbox A problem is \textbf{well-conditioned} if its condition
    number is modest and \textbf{ill-conditioned} if the condition
    number is large. 

    \redbox The condition number of $a\mapsto a^n$ is $\kappa(a) = n$,
    and the condition number of $a\mapsto a - b$ is
    $\frac{a}{a-b}$ (so subtracting $b$ is ill-conditioned near
    $b$---this is called \textbf{catastrophic cancellation}).

    \redbox The relative roundoff error between a non-extreme real
    number and the nearest \jul{T}-representable value is no more than
    the \textbf{machine epsilon} ($\epsilon_{\text{mach}}$) of the
    floating point type \jul{T}.

    \redbox An algorithm which solves a problem with error much
    greater than $\kappa \epsilon_{\text{mach}}$ is \textbf{unstable}. An
    algorithm is unstable if at least one of the steps it performs is
    ill-conditioned. If every step of an algorithm is
    well-conditioned, then the algorithm is \textbf{stable}.

    \redbox The condition number of an matrix $A$ is defined to be the
    maximum condition number of the function
    $\mathbf{x} \mapsto A\mathbf{x}$ over its domain. The condition
    number is equal to the ratio of the largest to the smallest
    singular value of $A$. 
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Numerical Computation: PRNGs},
    colframe = DarkRed!75!white, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softred, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]

    \redbox A \textbf{pseudorandom number generator} (PRNG) is an
    algorithm for generating a deterministic sequence of numbers which
    is intended to share properties with a sequence of random
    numbers. The PRNG's initial value is called its \textbf{seed}.

    \redbox The \textbf{linear congruential generator}: fix
    positive integers $M$, $a$, and $c$, and consider a seed
    $X_0 \in \{0,1,\ldots,M-1\}$. We return the sequence
    $X_0, X_1, X_2, \ldots$, where $X_n = \mathrm{mod}(aX_{n-1}+c,M)$ for
    $n \geq 1$.

    \redbox The \textbf{period} of a PRNG is the minimum length of a
    repeating block. A long period is a desirable property of a PRNG,
    and a very short period is typically unacceptable. 

    \redbox \textbf{Frequency tests} check whether blocks of terms
    appear with the appropriate frequency (for example, we can check
    whether $a_{2n} > a_{2n-1}$ for roughly half of the values of
    $n$). 
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Numerical Computation:
      Automatic Differentiation},
    colframe = DarkRed!75!white, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softred, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]

    \redbox A \textbf{dual number} is an object that can be
    substituted into a function $f$ to yield both the value of the
    function and its derivative at a point $x$.

    \redbox If $f$ is a function which can act on matrices, then
    $\left[\begin{smallmatrix} x & 1 \\ 0 &
        x \end{smallmatrix}\right]$ represents a dual number at $x$, since
    $f\left(\left[\begin{smallmatrix} x & 1 \\ 0 &
        x \end{smallmatrix}\right]\right) = \left[\begin{smallmatrix} f(x) & f'(x) \\ 0 &
        f(x) \end{smallmatrix}\right]$. (This identity is true for
    any function $f$ which can be defined as a limit of polynomial
    functions, since it can be checked to hold for $f + g$ and $fg$
    whenever it holds for $f$ and $g$, and it holds for the identity
    function). 

    \redbox To find the derivative of \jul{f} with automatic
    differentiation, every step in the computation of \jul{f} must be
    dual-number-aware. See the packages \jul{ForwardDiff} (for Julia)
    and \mintinline{python}{autograd} (for Python). 
    
  \end{tcolorbox}

    \begin{tcolorbox}[title={\fontspec{American Typewriter Bold} Numerical Computation: Optimization},
    colframe = DarkRed!75!white, breakable, before upper = \setcounter{boxcounter}{0},
    colback = softred, boxsep = 0.5pt, left = 2mm, right = 2mm, 
    enhanced, parbox = false, drop fuzzy shadow]

    \redbox \textbf{Gradient descent} seeks to minimize
    $f: \R^n \to \R$ by repeatedly stepping in $f$'s direction of
    maximum decrease. We begin with a value $\mathbf{x}_0 \in \R^n$
    and repeatedly update using the rule
    $\mathbf{x}_{n+1} = \mathbf{x}_{n}- \epsilon \nabla
    f(\mathbf{x}_{n-1})$, where $\epsilon$ is the \textbf{learning
      rate}.  We fix a small number $\tau > 0$ and stop when
    $|\nabla f(\mathbf{x}_n)| < \tau$.

    \redbox A function is strictly \textbf{convex} if its Hessian is
    positive semidefinite everywhere. A strictly convex function has
    at most one local minimum, and any local minimum is also a global
    minimum. Gradient descent will find the global minimum for a
    convex function, but for non-convex functions it can get stuck in
     a non-global local minimum.

    \redbox Algorithms similar to gradient descent but with usually
    faster convergence: \textbf{conjugate gradient, BFGS, L-BFGS}.
  \end{tcolorbox}
  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Counting}, colframe = Purple!75!black, breakable,
    before upper = \setcounter{boxcounter}{0}, colback = Purple!5!white,
    boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced, parbox = false,
    drop fuzzy shadow]

    \pbox \textbf{Fundamental principle of counting:} If one
    experiment has $m$ possible outcomes, and if a second experiment
    has $n$ possible outcomes for each of the outcomes in the first
    experiment, then there are $mn$ possible outcomes for the pair of
    experiments.

    \pbox The number of ways to arrange $n$ objects in order is
    $n! = 1 \cdot 2 \cdot 3 \cdot \cdots \cdot n$ (read $n$
    \textbf{factorial}).

    \pbox \textbf{Permutations}: if $S$ is a set with $n$
    elements, then there are $\frac{n!}{(n-r)!}$ \textit{ordered}
    $r$-tuples of distinct elements of $S$.

    \pbox \textbf{Combinations}: The number of
    $r$-element subsets of an $n$-element set is
    $\binom{n}{r} = \frac{n!}{r!(n-r)!}$.
    
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Probability Spaces}, colframe = Purple!75!black,
    breakable, before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    parbox = false, drop fuzzy shadow]
    
    \pbox Given a random experiment, the set of possible outcomes
    is called the \textbf{sample space} $\Omega$, like
    $\{(\texttt{H},\texttt{H}), (\texttt{H},\texttt{T}),
    (\texttt{T},\texttt{H}), (\texttt{T},\texttt{T})\}$.

    \pbox We associate with each outcome $\omega \in \Omega$ a
    \textbf{probability mass}, denoted $m(\omega)$. For example,
    $m((\texttt{H},\texttt{T})) = \frac{1}{4}$. 

    \pbox In a random experiment, an \textbf{event} is a predicate
    that can be determined based on the outcome of the experiment
    (like ``first flip turned up heads''). Mathematically, an event is
    a subset of $\Omega$ (like $\{(\texttt{H},\texttt{H}),
    (\texttt{H},\texttt{T})\}$). 

    \pbox Basic set operations $\cup$, $\cap$, and ${}\complement$
    correspond to disjunction, conjunction, and negation of events:
    \begin{enumerate}[itemsep=0pt,topsep=3pt,labelsep=2pt]
    \item The event that $E$ happens or $F$ happens is $E \cup
      F$.
    \item The event that $E$ happens and $F$ happens is $E \cap
      F$.
    \item The event that $E$ does not happen is $E\complement$. 
    \end{enumerate}

    \pbox If $E$ and $F$ cannot both occur
    (that is, $E \cap F = \emptyset$), we say that $E$ and $F$ are
    \textbf{mutually exclusive} or \textbf{disjoint}.

    \pbox If $E$'s occurrence implies $F$'s occurrence, then $E
    \subset F$. 
    
    \pbox The probability $\P(E)$ of an event $E$ is the sum of
    the probability masses of the outcomes in that event. The
    domain of $\P$ is $2^\Omega$, the set of all subsets of $\Omega$.

    \pbox The pair $(\Omega,\P)$ is called a probability
    space. The fundamental probability space properties are 
    \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep=2pt]
    \item $\P(\Omega) = 1$ --- ``something has to happen''
    \item $\P(E) \geq 0$ --- ``probabilities are non-negative''
    \item $\P(E \cup F) = \P(E) + \P(F)$ if $E$ and $F$ are
      mutually exclusive --- ``probability is additive''. 
    \end{enumerate}

    \pbox Other properties which follow from the fundamental ones: 
    \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep=2pt]
    \item $\P(\emptyset) = 0$ 
    \item $\P(E\complement) = 1 - \P(E)$
    \item $E \subset F \implies \P(E) \leq \P(F)$ (monotonicity) 
    \item $\P(E \cup F) = \P(E) + \P(F) - \P(E \cap F)$ (principle of
      inclusion-exclusion). 
    \end{enumerate}  
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Random Variables}, colframe = Purple!75!black,
    breakable, before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    parbox = false, drop fuzzy shadow]

    \pbox A \textbf{random variable} is a number which depends on
    the result of a random experiment (one's lottery winnings, for
    example). Mathematically, a random variable is a function $X$ from
    the sample space $\Omega$ to $\R$. 

    \pbox The \textbf{distribution} of a random variable $X$ is
    the probability measure on $\R$ which maps each set $A\subset \R$
    to $\P(X \in A)$. The probability mass function of the
    distribution of $X$ may be obtained by pushing forward the
    probability mass from each $\omega \in \Omega$:

    \begin{center}
      \includegraphics[width=5cm]{figures/distribution}
    \end{center}

    \pbox The \textbf{cumulative distribution function} (CDF) of a
    random variable $X$ is the function $F_X(x) = \P(X \leq x)$.

    \begin{center}
      \includegraphics[width=5cm]{figures/cdf}
    \end{center}

    \pbox The \textbf{joint distribution} of two random variables
    $X$ and $Y$ is the probability measure on $\R^2$ which maps
    $A \subset \R^2$ to $\P((X,Y) \in A)$. The probability mass
    function of the joint distribution is $m_{(X,Y)}(x,y) = \P(X = x
    \text{ and }Y = y)$. 
    
  \end{tcolorbox}
  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Conditional Probability}, colframe = Purple!75!black,
    breakable, before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    parbox = false, drop fuzzy shadow]

    \pbox Given a probability space $\Omega$ and an event $E
    \subset \Omega$, the \textbf{conditional probability measure}
    given $E$ is an updated probability measure on $\Omega$ which
    accounts for the information that the result $\omega$ of the
    random experiment falls in $E$: 
    \[
      \P(F \given E) = \frac{\P(F \cap E)}{\P(E)}
    \]

    \pbox The conditional probability mass function of $Y$ given
    $\{X = x\}$ is $m_{Y \given X = x}(y) = m_{X,Y}(x,y) / m_X(x)$.

    \pbox \textbf{Bayes' theorem} tells us how to update beliefs in
    light of new evidence. It relates the conditional probabilities
    $\P(A \given E)$ and $\P(E \given A)$:
    \[
      \P(A \given E) = \frac{\P(E \given A)\P(A)}{\P(E)} = \frac{\P(E
        \given A)\P(A)}{\P(E \given A)\P(A) + \P(E \given
        A^{\mathrm{c}})\P(A^{\mathrm{c}})}.
    \]

    \pbox Two events $E$ and $F$ are \textbf{independent} if $
    \P(E \cap F) = \P(E) \P(F)$.

    \pbox Two random variables $X$ and $Y$ are \textbf{independent} if
    the every pair of events of the form $\{X \in A\}$ and
    $\{Y \in B\}$ are independent, where $A \subset \R$ and
    $B \subset \R$.

    \pbox The PMF of the joint distribution of a pair of
    independent random variables factors as $m_{X,Y}(x,y) = m_X(x)
    m_Y(y)$: 

    \begin{center}
      \includegraphics[width=42mm]{figures/productmeasure}
    \end{center}

  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Expectation and Variance}, colframe = Purple!75!black,
    before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    parbox = false, drop fuzzy shadow]

    \pbox The \textbf{expectation} $\E[X]$ (or \textbf{mean} $\mu_X$) of
    a random variable $X$ is the \textit{probability-weighted average
      of $X$}: 
    \[
      \E[X] = \sum_{\omega \in \Omega} X(\omega) m(\omega) 
    \]

    \vspace{-2mm} 

    \pbox The expectation $\E[X]$ may be thought of as the value of a
    random game with payout $X$, or as the long-run average of $X$
    over many independent runs of the underlying experiment. The
    \textbf{Monte Carlo} approximation of $\E[X]$ is obtained by
    simulating the experiment many times and averaging the value of
    $X$.

    \pbox The expectation is the center of mass of the distribution
    of $X$:

    \vspace{-3mm}
    \begin{center}
      \includegraphics{figures/balance-point}
    \end{center}
    \vspace{-2mm}
    
    \pbox The expectation of a function of a discrete random variable
    (or two random variables) may be expressed in terms of the PMF
    $m_X$ of the distribution of $X$ (or the PMF $m_{(X,Y)}$ of the
    joint distribution of $X$ and $Y$): 
    \begin{align*}
      \E[g(X)] &= \sum_{x \in \R} g(x) m_X(x) \\
      \E[g(X,Y)] &= \sum_{(x,y)\in \R^2} g(x,y) m_{(X,Y)}(x,y). 
    \end{align*}

    \vspace{-2mm}

  \pbox Expectation is \textbf{linear}: if $c \in \R$ and $X$ and $Y$
  are random variables defined on the same probability space, then 
  \[
    \E[cX+Y] = c\,\E[X] + \E[Y] 
  \]
  
  \pbox The \textbf{variance} of a random variable is its average
  squared deviation from its mean. The variance measures how spread
  out the distribution of $X$ is. The \textbf{standard deviation}
  $\sigma(X)$ is the square root of the variance.
  
  \pbox Variance satisfies the properties, if $X$ and $Y$ are
  independent random variables and $a \in \R$: 
  \[
    \begin{array}{r@{\,}c@{\,}l}
      \Var(a X) &=& a^2 \Var X\\
      \Var(X+Y) &=& \Var(X) + \Var(Y)
    \end{array}
  \]

  \vspace{-2mm}

  \pbox The \textbf{covariance} of two random variables $X$ and $Y$ is
  the expected product of their deviations from their
  respective means $\mu_X = \E[X]$ and $\mu_Y = \E[Y]$: 
  \[
    \Cov(X,Y) = \E[ (X - \mu_X) (Y - \mu_Y)] = \E[XY] - \E[X] \E[Y]. 
  \]

  \pbox The covariance of two independent random variables is zero,
  but zero covariance does not imply independence.

  \pbox The \textbf{correlation} of two random variables is their
  normalized covariance:
  \[
    \Corr(X,Y) = \frac{\Cov(X,Y)}{\sigma(X)\sigma(Y)} \in [-1,1]
  \]
  
  \pbox The \textbf{covariance matrix} of a vector
  $\mathbf{X} = [X_1, \ldots, X_n]$ of random variables defined on the
  same probability space is defined to be the matrix $\Sigma$ whose
  $(i,j)$th entry is equal to $\Cov(X_i,X_j)$. If $\E[\mathbf{X}] =
  \bm{0}$, then $\Sigma = \E[\mathbf{X}\mathbf{X}\transpose]$.
\end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Continuous Distributions}, colframe = Purple!75!black,
    before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    breakable, parbox = false, drop fuzzy shadow, break at = 85pt/0pt]

    \pbox If $\Omega \subset \R^n$ and $\P(A) = \int_A f$, where
    $f\geq 0$ and $\int_{\R^n} f = 1$, then we call $(\Omega, \P)$ a
    \textbf{continuous probability space}.

    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/density}
    \end{center}

    \pbox The function $f$ is called a \textbf{density}, because it
    measures the amount of probability mass per unit volume at each
    point (2D volume = area, 1D volume = length).

    \pbox If $(X,Y)$ is a pair of random variables whose joint
    distribution has density $f_{X,Y}: \R^2 \to \R$, then the
    conditional distribution of $Y$ given the event $\{X=x\}$ has
    density $f_{Y \given X=x}$ defined by
    \[
      f_{Y\given \{X=x\}}(y)  = \frac{f_{X,Y}(x,y)}{f_X(x)}, 
    \]
    where $\displaystyle{f_X(x) = \int_{-\infty}^\infty f(x,y) \, \d y}$
    is the PDF of $X$. 

    \pbox If a random variable $X$ has density $f_X$ on $\R$, then 
    \[
      \E[g(X)] = \int_{\R} g(x) f_X(x) \, \d x. 
    \]

    \pbox \textbf{CDF sampling}: $F^{-1}(U)$ has CDF $F$ if
    $f_U =\boldsymbol{1}_{[0,1]}$. 
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Conditional Expectation}, colframe = Purple!75!black,
    before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    parbox = false, drop fuzzy shadow]

    \pbox The \textbf{conditional expectation} of a random variable
    given an event is the expectation of the random variable
    calculated with respect to the conditional probability measure
    given that event: if $(X,Y)$ has PMF $m_{X,Y}$, then 
    \[
      \E[Y \given X = x] = \sum_{y \in \R} y m_{Y \given X = x}(y), 
    \]
    where $m_{Y \given X = x}(y) = \frac{m_{X,Y}(x,y)}{m_X(x)}$. If
    $(X,Y)$ has pdf $f_{X,Y}$, then
    \[
      \E[Y \given X = x] = \int_\R y f_{Y \given X = x}(y)\, \d y. 
    \]

    \pbox The conditional expectation of a random variable $Y$ given
    another random variable $X$ is obtained by substituting $X$ for
    $x$ in the expression for the conditional expectation of $Y$ given
    $X = x$. Thus $\E[Y \given X]$ is a random variable.

    \pbox If $X$ and $Y$ are independent, then $\E[Y \given X] =
    \E[Y]$. If $Z$ is a function of $X$, then $\E[Z Y \given X] =
    Z\E[Y \given X]$. 

    \pbox The \textbf{law of iterated expectation}:
    $\E[\E[Y \given X]] = \E[Y]$. 
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Common Distributions}, colframe = Purple!75!black,
    before upper = \setcounter{boxcounter}{0}, colback =
    Purple!5!white, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced,
    breakable, parbox = false, drop fuzzy shadow, break at = 200pt/0pt]

    \pbox \textbf{Bernoulli} ($\operatorname{Ber}(p)$): A
    weighted coin flip.  

    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/bernoulli}
    \end{center}

    \pbox \textbf{Binomial} ($\operatorname{Bin}(n,p)$): A sum of $n$
    independent $\operatorname{Ber}(p)$'s. 
    
    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/binomial}
    \end{center}

    \pbox \textbf{Geometric} ($\operatorname{Geom}(p)$): Time to first
    success (1) in a sequence of independent
    $\operatorname{Ber}(p)$'s. 

    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/geometric}
    \end{center}

    \pbox \textbf{Poisson distribution} ($\operatorname{Poiss}(\lambda)$): Limit as $n\to\infty$ of $\text{Binomial}(n,\frac{\lambda}{n})$.

    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/poisson}
    \end{center}

    \pbox \textbf{Exponential distribution}
    ($\operatorname{Exp}(\lambda)$): Limit as $n\to\infty$ of
    distribution of $1/n$ times a Geometric$(\lambda/n)$. 

    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/exponential}
    \end{center}

    \pbox \textbf{Normal distribution} ($\mathcal{N}(\mu,\sigma^2)$):
    Limit as $n\to\infty$ of the distribution of
    $\frac{X_1+X_2+\cdots+X_n}{\sqrt{n}}$, for any independent
    sequence $X_1, \ldots, X_n$ of identically distributed random
    variables (i.i.d.) with $\E[X_1] = \mu$ and
    $\Var(X_1) = \sigma^2 < \infty$ (see Central Limit Theorem).

    \vspace{-3mm}
    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/gaussian}
    \end{center}
    \vspace{-3mm}

    \pbox \textbf{Multivariate normal distribution}
    ($\mathcal{N}(\boldsymbol{0},\Sigma)$): if
    $\bm{Z} = (Z_{1},Z_{2},\ldots,Z_{n})$ is a vector of independent
    $\mathcal{N}(0,1)$'s, $A$ is an $m\times n$ matrix of constants,
    and $\bm{\mu}\in \R^m$, then the vector
    \[
      \bm{X} = A\bm{Z} + \bm{\mu} 
    \]
    is \textbf{multivariate normal}. The covariance matrix of
    $\mathbf{X}$ is $\Sigma = AA'$.

    \begin{center}
      \includegraphics[width=0.8\textwidth]{figures/multinormal}
    \end{center}
    
  \end{tcolorbox}
  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Probability: Central Limit Theorem}, colframe = Purple!75!black,
    before upper = \setcounter{boxcounter}{0}, colback = Purple!5!white,
    boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced, breakable,
    parbox = false, drop fuzzy shadow]

    \pbox A sequence of random variables $X_1, X_2, \ldots,$
    \textbf{converges in probability} to $X$ if $\P(|X_n - X| >
    \epsilon) \to 0$ as $n\to\infty$, for any $\epsilon > 0$.

    \pbox A sequence $\nu_1, \nu_2, \ldots$ of probability measures on
    $\R^n$ converges to a probability measure $\nu$ if
    $\nu_n(A) \to \nu(A)$ for every set $A \subset \R^n$ with the
    property that $\nu(\partial A) = 0$ (intuitively, two
    measures are close if they put approximately the same amount
    of mass in approximately the same places). We say $X_n$
    \textbf{converges in distribution} to $\nu$ if the distribution of
    $X_n$ converges to $\nu$. 
    
    \pbox \textbf{Chebyshev's inequality}: if $X$ is a random variable
    with variance $\sigma^2 < \infty$, then $X$ differs from its mean
    by more than $k$ standard deviations with probability at most
    $k^{-2}$:
    \[
      \mathbb{P}(|X- \E[X]| > k \sigma) \leq \frac{1}{k^2}
    \]

    \pbox \textbf{Law of large numbers}: if $X_1, X_2, \ldots$ is
    a sequence of independent samples from a finite-variance
    distribution with mean $\mu$, then the sequence's running average
    converges in probability to $\mu$: for all $\epsilon > 0$,
    \[
      \P\left(
        \frac{X_1 + \cdots + X_n}{n} \notin [\mu - \epsilon,
        \mu+\epsilon ]
      \right) \to 0,  
    \]
    as $n\to\infty$.

    \pbox The PDF of a sum of $n$ independent samples from a
    finite-variance distribution looks increasingly bell-shaped as $n$
    increases, \textit{regardless of the distribution being sampled
      from}.

    \begin{center}
      \includegraphics[width=0.9\textwidth]{figures/CLT}
    \end{center}

    \pbox We define the \textbf{standardized running sum} of $X_1, X_2, \ldots$
    to have zero mean and unit variance for all $n \geq 1$: 
    \[
      S_n^* =  \frac{X_1 + X_2 + \cdots + X_n - n\mu}{\sigma\sqrt{n}}
    \]

    \pbox \textbf{Central limit theorem}: the sequence of standardized
    sums of an i.i.d.\ sequence of finite-variance random variables
    converges in distribution to $\mathcal{N}(0,1)$: for any interval
    $[a,b]$, we have
    \[
      \P(S_n^* \in [a,b]) \to \int_a^b
      \frac{1}{\sqrt{2\pi}}\e^{-t^2/2} \, \d t
    \]
    as $n\to\infty$.

    \pbox \textbf{Multivariate central limit theorem}: If
    $\mathbf{X}_1,\mathbf{X}_2, \ldots$ is a sequence of independent
    random vectors whose common distribution has mean $\bm{\mu}$ and
    covariance matrix $\Sigma$, then 
    \[
      \frac{\mathbf{X}_1 + \mathbf{X}_2 + \cdots + \mathbf{X}_n - n\boldsymbol{\mu}}{\sqrt{n}}
    \]
    converges in distribution to $\mathcal{N}(\boldsymbol{0},\Sigma)$.

    \pbox The central limit theorem explains the ubiquity of the
    normal distribution in statistics: many random quantities may be
    realized as a sum of a multitude of independent contributions.
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: Point estimation}, colframe = yellow!40!black, breakable,
  left = 2mm, right = 2mm, colback = softyellow, boxsep = 0.5pt,
  before upper = \setcounter{boxcounter}{0}, enhanced, parbox =
  false, drop fuzzy shadow]

  \ybox The central problem of statistics is to make inferences
  about a population or data-generating process based on the
  information in a finite sample drawn from the population.

  \vspace{1mm}

  \ybox \textbf{Parametric estimation} involves an assumption that
  the distribution of the data-generating process comes from a family
  of distributions parameterized by finitely many real numbers,
  while \textbf{nonparametric estimation} does
  not. \textit{Examples: An estimator that assumes the data are normally distributed is parametric, while histograms are nonparametric.}

  \vspace{1mm}
  
  \ybox \textbf{Point estimation} is the inference of a single
  real-valued feature of the distribution of the data-generating
  process (such as its mean, variance, or median).

  \vspace{1mm}

  \ybox A \textbf{statistical functional} is any function $T$ from
  the set of distributions to $[-\infty,\infty]$. An
  \textbf{estimator} $\widehat{\theta}$ is a random variable
  defined in terms of $n$ i.i.d.\ random variables, the purpose of
  which is to approximate some statistical functional of the random
  variables' common distribution. \textit{Example: Suppose that
    $T(\nu)$ = the mean of $\nu$, and that
    $\widehat{\theta} = (X_1 + \cdots + X_n)/n$.}

    \vspace{1mm}

  \ybox The empirical measure $\widehat{\nu}$ of $X_1, \ldots, X_n$
  is the probability measure which assigns mass $\frac{1}{n}$ to
  each sample's location. The \textbf{plug-in estimator} of
  $\theta = T(\nu)$ is obtained by applying $T$ to the empirical
  measure: $\widehat{\theta} = T(\widehat{\nu})$.

  \vspace{1mm}
  
  \ybox Given a distribution $\nu$ and a statistical functional
  $T$, let $\theta = T(\nu)$. The \textbf{bias} of an estimator of
  $\theta$ is the difference between the estimator's expected value
  and $\theta$. \textit{Example: The expectation of the sample mean
    $\widehat{\theta} = (X_1 + \cdots + X_n)/n$ is
    $\E(X_1 + \cdots + X_n)/n = \E[\nu],$ so the bias of the sample
    mean is zero.}

    \vspace{1mm}

  \ybox The \textbf{standard error}
  $\operatorname{se}(\widehat{\theta})$ of an estimator
  $\widehat{\theta}$ is its standard deviation.

  \vspace{0.5mm}

  \ybox An estimator is \textbf{consistent} if $\widehat{\theta}
  \to \theta$ in probability as $n\to\infty$.

  \vspace{0.5mm}

  \ybox The \textbf{mean squared error} of an
  estimator is defined to be 
  \[
    \mathrm{MSE}(\theta) = \E[(\widehat{\theta} - \theta)^2].
  \]

  \ybox MSE is equal to variance plus squared bias. Therefore, MSE
  converges to zero as the number of samples goes to $\infty$ if
  and only if variance and bias both converge to zero. 
\end{tcolorbox}

\begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: Confidence intervals}, colframe = yellow!40!black, breakable,
  left = 2mm, right = 2mm, colback = softyellow, boxsep = 0.5pt,
  before upper = \setcounter{boxcounter}{0}, enhanced, parbox =
  false, drop fuzzy shadow]

  \ybox Consider an unknown probability distribution $\nu$ from
  which we get $n$ independent samples $X_1, \ldots, X_n$, and
  suppose that $\theta$ is the value of some statistical functional
  of $\nu$. A \textbf{confidence interval} for $\theta$ is an
  interval-valued function of the sample data $X_1, \ldots, X_n$. A
  confidence interval has \textbf{confidence level} $1-\alpha$
  if it contains $\theta$ with probability at least $1-\alpha$.

  \vspace{1mm}
  
  \ybox If $\widehat{\theta}$ is unbiased, then $\left(\widehat{\theta}
    - k\operatorname{se}(\widehat{\theta}), \widehat{\theta}
    + k\operatorname{se}(\widehat{\theta})\right)$ is a $1-\frac{1}{k^2}$
  confidence interval, by Chebyshev's inequality. 

  \vspace{1mm}
  
  \ybox If $\widehat{\theta}$ is unbiased and approximately
  normally distributed, then
  $\left(\widehat{\theta}-1.96\operatorname{se}(\widehat{\theta}),
    \widehat{\theta}+1.96\operatorname{se}(\widehat{\theta})\right)$
  is an approximate 95\% confidence interval, since 95\% of the
  mass of the standard normal distribution is in the interval
  $[-1.96,1.96]$.

  \vspace{1mm}

  \ybox Let $I \subset \R$, and suppose that $T$ is a function from
  the set of distributions to the set of real-valued functions on
  $I$. A $1-\alpha$ \textbf{confidence band} for $T(\nu)$ is pair
  of random functions $y_{\textrm{min}}$ and $y_{\textrm{max}}$
  from $I$ to $\R$ defined in terms of $n$ independent samples from
  $\nu$ and having
  $y_{\textrm{min}} \leq T(\nu) \leq y_{\textrm{max}}$ everywhere
  on $I$ with probability at least $1-\alpha$.
\end{tcolorbox}

\begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: Empirical CDF convergence}, colframe =
  yellow!40!black, breakable, left = 2mm, right = 2mm, colback =
  softyellow, boxsep = 0.5pt, before upper =
  \setcounter{boxcounter}{0}, enhanced, parbox = false, drop fuzzy
  shadow]

  \ybox Statistics is predicated on the idea that a distribution is
  \begin{insetfigure}{\includegraphics[width=0.4\textwidth]{figures/uniformcdf}}[][8pt]  
    well-approximated by independent samples therefrom. The
    \textbf{Glivenko-Cantelli theorem} is one formalization of this
    idea: If $F$ is the CDF of a distribution $\nu$ and
    $\widehat{F}_n$ is the CDF of the empirical distribution
    $\widehat{\nu}_n$ of $n$ samples from $\nu$, then $F_n$
    converges to $F$
  \end{insetfigure}

  \vspace{-4pt}
  
  along the whole number line:

  \vspace{-3mm}
  
  \[
    \max_{x\in \R} |F(x) - \widehat{F}_n(x)| \to 0 \quad \text{as
    }n \to \infty, 
  \]

  \vspace{-2mm} 
  
  in probability.
    
  \ybox The \textbf{Dvoretzky-Kiefer-Wolfowitz inequality} (DKW)
  says that the graph of $\widehat{F}_n$ lies in the
  $\epsilon$-band around the graph of $F$ with probability at
  least $1 - 2\e^{-2n\epsilon^2}$. 
\end{tcolorbox}

\begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: Bootstrapping}, colframe = yellow!40!black, breakable,
  left = 2mm, right = 2mm, colback = softyellow, boxsep = 0.5pt,
  before upper = \setcounter{boxcounter}{0}, enhanced, parbox =
  false, drop fuzzy shadow]
  
  \ybox \textbf{Bootstrapping} is the use of simulation to
  approximate the value of the plug-in estimator of a statistical
  functional which is expressed in terms of independent samples
  from $\nu$.

  \vspace{-1mm}

  \textit{Example: if $\theta = T(\nu)$ is the variance of the
    median of 3 independent samples from $\nu$, then the bootstrap
    estimate of $\theta$ is obtained as a Monte Carlo approximation
    of $T(\widehat{\nu})$: we sample 3 times (with replacement)
    from $\{X_1, \ldots, X_n\}$, record the median, repeat $B$
    times for $B$ large, and take the sample variance of the
    resulting list of $B$ numbers.} 

  \ybox The bootstrap approximation of $T(\widehat{\nu})$ may be
  made as close to $T(\widehat{\nu})$ as desired by choosing $B$
  large enough. The difference between $T(\nu)$ and
  $T(\widehat{\nu})$ is likely to be small if $n$ is large (that
  is, if many samples from $\nu$ are available).

  \ybox The bootstrap is useful for computing standard errors,
  since the standard error of an estimator is often infeasible to
  compute analytically but conducive to Monte Carlo approximation.
\end{tcolorbox}

\begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: Maximum likelihood estimation}, colframe =
  yellow!40!black, breakable, left = 2mm, right = 2mm, colback =
  softyellow, boxsep = 0.5pt, before upper =
  \setcounter{boxcounter}{0}, enhanced, parbox = false, drop fuzzy
  shadow]

  \ybox Maximum likelihood estimation is a general approach for
  proposing an estimator. Consider a parametric family
  $\{f_{\bdtheta}(x)\,:\, \bdtheta \in \R^d\}$ of PDFs or PMFs. Given
  $\mathbf{x} \in \R^n$, the \textbf{likelihood}
  $\mathcal{L}_{\mathbf{x}}: \R^d \to \R$ is defined by
  \[
    \mathcal{L}_{\mathbf{x}}(\bdtheta) = f_{\bdtheta}(x_{1})f_{\bdtheta}(x_{2})\cdots
    f_{\bdtheta}(x_{n}).
  \]
  If $\mathbf{X}$ is a vector of $n$ independent samples drawn from
  $f_{\bdtheta}(x)$, then $\mathcal{L}_{\mathbf{X}}(\bdtheta)$ is
  small or zero when $\bdtheta$ is not in accordance with the
  observed data.

  \textit{Example: Suppose $x\mapsto f(x;\theta)$ is the
    density of a uniform random variable on $[0,\theta]$. We
    observe four samples drawn from this distribution:
    $1.41, 2.45, 6.12$, and $4.9$. Then the likelihood of
    $\theta = 5$ is zero, and the likelihood of $\theta = 10^6$ is
    very small.}
  
  \ybox The \textbf{maximum likelihood estimator} is
  \[
    \widehat{\bdtheta}_{\mathrm{MLE}} = \argmax_{\bdtheta \in
      \R^d}\mathcal{L}_\mathbf{X}(\bdtheta).
  \]
  Equivalently,
  $\widehat{\bdtheta}_{\mathrm{MLE}} = \argmax_{\bdtheta \in
    \R^d}\mathcal{\ell}_\mathbf{X}(\bdtheta)$, where
  $\ell _\mathbf{x}(\bdtheta)$ denotes the logarithm of
  $\mathcal{L} _\mathbf{x}(\bdtheta)$.

  \textit{Example: Suppose that $x\mapsto f(x;\mu,\sigma^2)$ is the
    normal density with mean $\mu$ and variance $\sigma^2$. Then
    the maximum likelihood estimator is the minimizer of the
    log-likelihood
    \[
      -\frac{n}{2}\log 2\pi - n \log \sigma -
      \frac{(X_1-\mu)^2}{2\sigma^2} - \cdots - \frac{(X_n - \mu)^2}{2\sigma^2}
    \]
    Setting the derivatives with respect to $\mu$ and $\sigma^2$
    equal to zero, we find
    $\mu = \overline{X} = \frac{1}{n}(X_1+\cdots+X_n)$ and
    $\sigma^2 = \frac{1}{n}((X_1-\overline{X})^2 + \cdots +
    (X_n-\overline{X})^2)$. So the maximum likelihood estimators
    agree with the plug-in estimators. }

  \ybox MLE enjoys several nice properties: under certain
  regularity conditions, we have (stated for $\theta \in \R^1$): 
  \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
  \item \textbf{Consistency}:
    $\E[(\widehat{\theta} - \theta)^2] \to 0$ as the number of
    samples goes to $\infty$.
  \item \textbf{Asymptotic normality}:
    $(\widehat{\theta} - \theta)/\sqrt{\Var \widehat{\theta}}$
    converges to $\mathcal{N}(0,1)$ as the number of samples goes
    to $\infty$.
  \item \textbf{Asymptotic optimality}: the MSE of the MLE
    converges to 0 approximately as fast as the MSE of any other
    consistent estimator.
  \end{enumerate}

\ybox Potential difficulties with the MLE: 
  \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
  \item \textbf{Computational challenges}. It might be hard to work
    out where the maximum of the likelihood occurs, either
    analytically or numerically.
  \item \textbf{Misspecification}. The MLE may be inaccurate if the
    distribution of the samples is not in the specified parametric
    family. 
  \item \textbf{Unbounded likelihood}. If the likelihood function
    is not bounded, then $\widehat{\theta}$ is not well-defined.
  \end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: Hypothesis testing}, colframe = yellow!40!black, breakable,
  left = 2mm, right = 2mm, colback = softyellow, boxsep = 0.5pt,
  before upper = \setcounter{boxcounter}{0}, enhanced, parbox =
  false, drop fuzzy shadow]

  \ybox \textbf{Hypothesis testing} is a disciplined framework for
  adjudicating whether observed data do not support a given
  hypothesis.

  \ybox Consider an unknown distribution from which we will observe
  $n$ samples $X_1, \ldots X_n$. 
  \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
  \item We state a hypothesis $H_0$--called the
    \textbf{null hypothesis}--about the distribution. 
  \item We come up with a \textbf{test statistic} $T$, which is a
    function of the data $X_1, \ldots X_n$, for which we can evaluate
    the distribution of $T$ assuming the null hypothesis. 
  \item We give an \textbf{alternative hypothesis} $H_{\mathrm{a}}$
    under which $T$ is expected to be significantly different from
    its value under $H_0$. 
  \item We give a significance level $\alpha$ (like 5\% or 1\%), and
    based on $H_{\mathrm{a}}$ we determine a set of values for
    $T$---called the \textit{critical region}---which $T$ would be
    in with probability at most $\alpha$ under the null hypothesis.
  \item \textbf{After setting $\boldsymbol{H_0}$,
      $\boldsymbol{H_{\mathrm{a}}}$, $\bdalpha$, $\boldsymbol{T}$, and the
      critical region}, we run the experiment, evaluate $T$ on the
    samples we get, and record the result as $t_\mathrm{obs}$.
  \item If $t_\mathrm{obs}$ falls in the critical region, we reject
    the null hypothesis. The corresponding \textbf{\textit{p}-value}
    is defined to be the minimum $\alpha$-value which would have
    resulted in rejecting the null hypothesis, with the critical
    region chosen in the same way*.
  \end{enumerate}
  
  \textit{Example: Muriel Bristol claims that she can tell by taste
    whether the tea or the milk was poured into the cup first. She
    is given eight cups of tea, four poured milk-first and four
    poured tea-first.}

  \textit{We posit a null hypothesis that she isn't able to discern
    the pouring method, under which the number of cups identified
    correctly is 4 with probability $1/\binom{8}{4} \approx 1.4\%$
    and at least 3 with probability $17/70 \approx 24\%$.
    Therefore, at the 5\% significance level, only a correct
    identification of all the cups would give us grounds to reject
    the null hypothesis. The $p$-value in that case would be
    1.4\%.}

  \ybox Failure to reject the null hypothesis is not necessarily
  evidence \textit{for} the null hypothesis. The \textbf{power} of
  a hypothesis test is the conditional probability of rejecting the
  null hypothesis given that the alternative hypothesis is true. A
  $p$-value may be low either because the null hypothesis is true
  or because the test has low power. 

  \ybox The \textbf{Wald test} is based on the normal
  approximation. Consider a null hypothesis $\theta = 0$ and the
  alternative hypothesis $\theta \neq 0$, and suppose that
  $\widehat{\theta}$ is approximately normally distributed. The
  Wald test rejects the null hypothesis at the 5\% significance
  level if
  $|\widehat{\theta}| > 1.96 \operatorname{se}(\widehat{\theta})$.

  \ybox The \textbf{random permutation test} is applicable when
  the null hypothesis is that the mean of a given random variable
  is equal for two populations.
  \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
  \item We compute the difference between the sample means for the
    two groups.
  \item We randomly re-assign the group labels and compute the
    resulting sample mean differences. Repeat many times. 
  \item We check where the original difference falls in the sorted
    list of re-sampled differences. 
  \end{enumerate}

  \textit{Example: Suppose the heights of the Romero sons are 72,
    69, 68, and 66 inches, and the heights of the Larsen sons are
    70, 65, and 64 inches. Consider the null hypothesis that the
    expected heights are the same for the two families, and the
    alternative hypothesis that the Romero sons are taller on
    average (with $\alpha = 5\%$). We find that
    the sample mean difference of about 2.4 inches is larger than
    88.5\% of the mean differences obtained by resampling many
    times. Since 88.5\% < 95\%, we retain the null hypothesis.}

  \ybox If we conduct many hypothesis tests, then the probability
  of obtaining some false rejections is high
  (\url{xkcd.com/882}). This is called the \textbf{multiple testing
    problem}. The \textbf{Bonferroni method} is to reject the null
  hypothesis only for those tests whose $p$-values are less than
  $\alpha$ divided by the number of hypothesis tests being
  run. This ensures that the probability of having even one false
  rejection is less than $\alpha$, so it is very conservative.
\end{tcolorbox}


  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Theory}, colframe =
    MidnightBlue!75!white, before upper = \setcounter{boxcounter}{0},
    colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm,
    enhanced, breakable, parbox = false, drop fuzzy shadow, break at = 320pt/0pt]

    \bluebox \textbf{Statistical learning}: Given some samples from a
    probability space with an unknown probability measure, we seek to
    draw conclusions about the measure.

    \bluebox \textbf{Supervised learning}: $(\mathbf{X},Y)$ is drawn
    from an unknown probability measure $\P$ on a product space
    $\mathcal{X} \times \mathcal{Y}$, and we aim to predict $Y$ given
    $\mathbf{X}$, based on a i.i.d.\ collection of samples from $\P$ (the
    \textbf{training data}).

    \textit{Example: $\mathbf{X} = [X_1, X_2]$, where $X_1$ is the
      color of a banana, $X_2$ is the weight of the banana, and $Y$ is
      a measure of deliciousness. Values of $X_1, X_2,$ and $Y$ are
      recorded for many bananas, and they are used to predict $Y$ for
      other bananas whose $\mathbf{X}$ values are known.}

    \bluebox We call the components of $\mathbf{X}$
    \textit{features}, \textit{predictors}, or \textit{input
      variables}, and we call $Y$ the \textit{response variable} or
    \textit{output variable}.

    \bluebox A supervised learning problem is a \textbf{regression}
    problem if $Y$ is quantitative ($\mathcal{Y}\subset \R$) and a
    \textbf{classification} problem if $\mathcal{Y}$ is a set of
    labels.

    \bluebox To choose a prediction function
    $h: \mathcal{X} \to \mathcal{Y}$, we specify a
    \begin{enumerate}[(i),topsep=0pt,leftmargin=24pt,itemsep=0pt]
    \item a space $\mathcal{H}$ of candidate functions, and
    \item a \textbf{loss} (or \textit{risk}) \textbf{functional} $L$ from
      $\mathcal{H}$ to $\R$.
    \end{enumerate}
    The \textbf{target function} is $\argmin_{h \in
      \mathcal{H}}L(h)$. 

    \bluebox If the loss functional for a regression problem is
    \[
      L(h) = \E[(h(X)-Y)^2]
    \]
    and $\mathcal{H}$ contains
    $r(\mathbf{x}) = \E[Y \given \mathbf{X} = \mathbf{x}]$, then
    $r$ is the target function. If the loss functional for a
    classification problem is
    \[
      L(h) = \E\left[\boldsymbol{1}_{\{h(X) \neq
          Y\}}\right], 
    \]
    and $\mathcal{H}$ contains
    $G(\mathbf{x}) = \argmax_c\P(Y=c \given \mathbf{X} =
    \mathbf{x})$, then $G$ is the target function.

    \bluebox Since $\P$ is unknown, we must approximate the target
    function with a function $\widehat{h}$ whose values can be
    computed from the training data. A \textbf{learner} is a function
    which takes a set of training data and returns a prediction
    function $\widehat{h}$.

    \bluebox The \textbf{empirical probability measure} on
    $\mathcal{X} \times \mathcal{Y}$ is the measure which assigns a
    probability mass of $\frac{1}{n}$ to the location of each training
    sample
    $(\mathbf{X}_1,Y_1), (\mathbf{X}_2,Y_2), \ldots,
    (\mathbf{X}_n,Y_n)$. The \textbf{empirical risk} of a candidate
    function $h$ is the risk functional evaluated with respect to the
    empirical measure of the training data. The \textbf{empirical risk
      minimizer} (ERM) is the function which minimizes empirical risk.

    \bluebox \textbf{Generalization error} (or \textit{test} error) is
    the difference between empirical risk and the actual value of the
    risk functional.

    \bluebox The ERM can \textbf{overfit}, meaning that test error and
    $L(\widehat{h})$ are large despite small empirical risk. 

    \textit{Example: if $\mathcal{H}$ is the space of polynomials and
      no two training samples have the same $\mathbf{x}$ values, then
      then there are functions in $\mathcal{H}$ which have zero
      empirical risk.}

    \begin{center}
      \includegraphics[width=0.7\textwidth]{figures/overfit}
    \end{center}
    
    \bluebox Mitigate overfitting with \textbf{inductive bias}:
    \begin{enumerate}[(i),topsep=0pt,leftmargin=24pt,itemsep=0pt]
    \item Use a restrictive class $\mathcal{H}$ of candidate
      functions. 
    \item \textbf{Regularize}: add a term to the loss functional
      which penalizes complexity. 
    \end{enumerate}

    \bluebox Inductive bias can lead to \textbf{underfitting}:
    relevant relations are missed, so both training and test error are
    larger than necessary. The tension between the costs of high
    inductive bias and the costs of low inductive bias is called the
    \textbf{bias-complexity} (or \textit{bias-variance})
    \textbf{tradeoff}.

    \bluebox \textbf{No-free-lunch theorem}: all learners are equal on
    average (over all possible problems), so inductive bias
    appropriate to a given type of problem is essential to have an
    effective learner for that type of problem.
  \end{tcolorbox}

  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Kernel density estimation}, colframe =
    MidnightBlue!75!white, before upper = \setcounter{boxcounter}{0},
    colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm,
    enhanced, breakable, parbox = false, drop fuzzy shadow, break at = 220pt/0pt]

    \bluebox Given $n$ samples $X_1, \ldots, X_n$ from a distribution
    with density $f$ on $\R$, we can estimate the PDF of the
    distribution by placing $1/n$ units of probability mass in a small
    pile around each sample.

    \bluebox We choose a \textbf{kernel} function for the shape of
    each pile:

    \begin{center}
      \includegraphics[width=0.65\textwidth]{figures/pile}
    \end{center}

    \vspace{-1mm}

    \bluebox The width of each pile is specified by a
    \textbf{bandwidth} $\lambda$: $D_\lambda(u) =
    \frac{1}{\lambda}D\left(\frac{u}{\lambda}\right)$.

    \bluebox The \textbf{kernel density estimator} with bandwidth
    $\lambda$ is the sum of the piles at each sample: 
    \[
      \widehat{f}_\lambda(x) =
      \frac{1}{n}\sum_{i=1}^{n}D_\lambda(x-X_i). 
    \]

    \bluebox To choose a suitable bandwidth, we seek to minimize the
    \textbf{integrated squared error} (ISE)
    $L(f) = \int_{\R} (f-\widehat{f})^2$. 

    \bluebox We approximate the minimizer of $L$ with the minimizer of
    the \textbf{cross-validation loss estimator}
    \[
      J(f) = \int_{\R} \widehat{f}_\lambda^{\:2} -
      \frac{2}{n}\sum_{i=1}^{n}\widehat{f}_\lambda^{\:(-i)}(X_i), 
    \]
    where $\widehat{f}_\lambda^{\:(-i)}$ is the KDE with the $i$th
    sample omitted. 

    \bluebox If $f$ is a density on $\R^2$, then we use the KDE
    \[
      \widehat{f}_\lambda(x,y) =
      \frac{1}{n}\sum_{i=1}^{n}D_\lambda(x-X_i)D_\lambda(y-Y_i). 
    \]

    \begin{center}
      \twographics[width=\textwidth]{figures/kde-small-surface}%
                  [width=\textwidth]{figures/kde2d}
    \end{center}

    \bluebox \textbf{Stone's theorem} says that the ratio of the CV
    ISE to the optimal-$\lambda$ ISE converges to $1$ in probability
    as $n\to\infty$. Also, the optimal $\lambda$ goes to 0 like
    $\frac{1}{n^{1/5}}$, and the minimal ISE goes to 0 like
    $\frac{1}{n^{4/5}}$.

    \bluebox The \textbf{Nadaraya-Watson} nonparametric regression
    estimator $\widehat{r}(x)$ computes $\E[Y \given X = x]$ with
    respect to the estimated density
    $\widehat{f}_\lambda$. Equivalently, we average the $Y_i$'s,
    weighted according to horizontal distance from $x$:
    \[
      \sum_{i=1}^{n}Y_iD(x-X_i)\bigg/ \sum_{i=1}^{n} D(x-X_i). 
    \]
  \end{tcolorbox}

    \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Parametric regression}, colframe =
    MidnightBlue!75!white, before upper = \setcounter{boxcounter}{0},
    colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm,
    enhanced, breakable, parbox = false, drop fuzzy shadow]

    \bluebox \textbf{Parametric regression} uses a family
    $\mathcal{H}$ of candidate functions which is indexed by finitely
    many parameters. 

    \bluebox \textbf{Linear regression} uses the set of affine
    functions:
    $\mathcal{H} = \{\mathbf{x}\mapsto \beta_0 +
    [\beta_1,\ldots,\beta_p]\cdot \mathbf{x} \,:\,
    \beta_0,\ldots\beta_p \in \R\}$. 

    \bluebox We choose the parameters to minimize a risk function,
    customarily the \textbf{residual sum of squares}: 
    \[
      \operatorname{RSS}(\bdbeta) = \sum_{i=1}^n (y_i - \beta_0 - \bdbeta \cdot
      \mathbf{x}_i)^2 = |\mathbf{y} - X \bdbeta|^2, 
    \]
    where $\mathbf{y} = [y_1, \ldots, y_n]$,
    $\bdbeta = [\beta_0, \ldots, \beta_p]$, and $X$ is an
    $n \times (p+1)$ matrix whose $i$th row is a 1 followed by the
    components of $\mathbf{x}_i$.

    \begin{center}
      \includegraphics[width=0.75\textwidth]{figures/linreg-rss}
    \end{center}

    \bluebox The RSS minimizer is $\bdbeta = (X\transpose X)^{-1}
    X'Y$.

    \bluebox We can use the linear regression framework to do
    \textbf{polynomial regression}, since a polynomial is linear in
    its coefficients: we supplement the list of features with
    products of the original regressors. 

    \bluebox Regularizing linear regression by penalizing the sum of the squares of the regression coefficients is called \textbf{ridge} regression, while penalizing the sum of the absolute values of the coefficients is called \textbf{lasso} regression.

  \end{tcolorbox}
  
  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Optimal classification}, colframe =
    MidnightBlue!75!white, before upper = \setcounter{boxcounter}{0},
    colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm,
    enhanced, breakable, parbox = false, drop fuzzy shadow]
    
    \bluebox Consider a classification problem with feature set
    $\mathcal{X}$ and class set $\mathcal{Y}$. For each
    $y\in \mathcal{Y}$, we define $p_y = \P(Y = y)$ and let $f_y$ be
    the conditional PMF or PDF of $\mathbf{X}$ given $\{Y = y\}$
    ($y$'s \textbf{class conditional distribution}).

    \bluebox Given a prediction function (or \textbf{classifier}) $h$
    and an enumeration of the elements of $\mathcal{Y}$ as
    $\{y_1, y_2, \ldots, y_{|\mathcal{Y}|}\}$, we define the
    (normalized) \textbf{confusion matrix} of $h$ to be the
    $|\mathcal{Y}| \times |\mathcal{Y}| $ matrix whose $(i,j)$th entry
    is $\P(h(\mathbf{X}) = y_i \given Y = y_j)$.

    \bluebox If $\mathcal{Y} = \{-,+\}$, the conditional probability
    of correct classification given a positive sample is the
    \textbf{detection rate} (DR), while the conditional probability of
    incorrect classification given a negative sample is the
    \textbf{false alarm rate} (FAR).

    \bluebox The \textbf{precision} of a classifier is the conditional
    probability that a sample is positive given that the classifier
    predicts positive, and \textbf{recall} is a synonym of detection
    rate.

    \bluebox The Bayes classifier
    $G(\mathbf{x}) = \argmax_y p_yf_y(\mathbf{x})$ minimizes the
    misclassification probability but gives equal weight to both types
    of misclassification.

    \bluebox The \textbf{likelihood ratio test} generalizes the Bayes
    classifier by allowing a variable tradeoff between false-alarm
    rate and detection rate: given $t > 0$, we say
    $h_t(\mathbf{x}) = -1$ if $f_+(\mathbf{x})/f_-(\mathbf{x}) < t$
    and $h_t(\mathbf{x}) = 1$ otherwise.
    
    \bluebox The \textbf{Neyman-Pearson lemma} says that no classifier does
    better on both false alarm rate and detection rate than $h_t$.
    
    \bluebox The \textbf{receiver operating characteristic} of $h_t$
    is the curve

    \vspace{-1mm}

    \begin{insetfigure}{\includegraphics[width=0.4\textwidth]{figures/roc}}[][8pt] 
      ${\footnotesize \{(\mathrm{FAR}(h_t), \mathrm{DR}(h_t)) \,: t
        \in [0,\infty]\}}$. The AUROC (area under the ROC) is close to
      1 for an excellent classifier and close to $\frac{1}{2}$ for a
      worthless one. NP says that no classifier is above the ROC.  We
      choose a point on the ROC curve based on context-specific
      considerations.
    \end{insetfigure}
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: QDA, LDA, Naive Bayes}, colframe
    = MidnightBlue!75!white, before upper =
    {\setcounter{boxcounter}{0}\setlength{\abovedisplayskip}{3pt}%
      \setlength{\belowdisplayskip}{3pt}}, colback = softblue, boxsep = 0.5pt,
    left = 2mm, right = 2mm, enhanced, breakable, parbox = false, drop
    fuzzy shadow]

    \bluebox \textbf{Quadratic discriminant analysis} (QDA) is a
    classification algorithm which uses the training data to estimate
    the mean $\bdmu_y$ and covariance matrix $\Sigma_y$ of each class
    conditional distribution:
    \begin{align*}
      \widehat{\bdmu}_y &= \mathrm{mean}(\{\mathbf{x}_i \,: \, y_i = y\}) \\
      \widehat{\Sigma}_y &= \mathrm{mean}(\{(\mathbf{x}_i - \widehat{\bdmu}_y)
                           (\mathbf{x}_i - \widehat{\bdmu}_y)' \,: \, y_i = y\}). 
    \end{align*}
    Each distribution is assumed to be multivariate normal 
    ($\mathcal{N}(\widehat{\bdmu}_y,\widehat{\Sigma}_y)$) and the
    classifier $h(y) = \argmax_y
    \widehat{p}_y\widehat{f}_y(\mathbf{x})$ is proposed (where
    $\{\widehat{p}_y\,:\,y \in \mathcal{Y}\}$ are the class
    proportions from the training data). 

    \bluebox \textbf{Linear discriminant analysis} (LDA) is the same
    as QDA except the class covariance matrices are assumed to be
    equal and are estimated using all of the data, not just
    class-specific samples. 

    \bluebox QDA and LDA are so named because they yield class
    prediction boundaries which are quadric surfaces and hyperplanes,
    respectively.

    \bluebox A \textbf{Naive Bayes} classifier assumes that the
    features are conditionally independent given $Y$: 
    \[
      f_y(x_1, \ldots, x_p) = f_{y,1}(x_1)\cdots f_{y,d}(x_p), 
    \]

    \vspace{-2mm} 
    
    for some $f_{y,1} ,\ldots f_{y,p}$. Assumption-satisfying examples:

    \vspace{-2.5mm} 
    
    \begin{center}
      \includegraphics[width=0.95\textwidth]{figures/QDA-LDA-NB}
    \end{center}
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Logistic regression}, colframe
    = MidnightBlue!75!white, before upper =
    {\setcounter{boxcounter}{0}\setlength{\abovedisplayskip}{2pt}%
      \setlength{\belowdisplayskip}{2pt}}, colback = softblue, boxsep = 0.5pt,
    left = 2mm, right = 2mm, enhanced, breakable, parbox = false, drop
    fuzzy shadow]
  
      \bluebox \textbf{Logistic regression} for binary classification
      estimates $r(\mathbf{x}) = \P(Y = 1 \given X = x)$ as a
      \textit{logistic function} of a linear function of $\mathbf{x}$:

      
      \begin{insetfigure}{\includegraphics[width=0.37\textwidth]{figures/sigmoid}}[][5pt]
      \[
        \widehat{r}(\mathbf{x}) = \sigma(\alpha + \bdbeta \cdot
        \mathbf{x}), 
      \]    
      where    
      \[
        \sigma(x) = \frac{1}{1+\e^{-x}}. 
      \]
    \end{insetfigure}

    \vspace{1mm}
  
    \bluebox We choose $\alpha$ and $\beta_1, \ldots, \beta_p$ to
    minimize the risk  
    \[
      L(r) = \sum_{i=1}^{n} \left[y_i \log \frac{1}{r(x_i)} + (1-y_i)\log\frac{1}{1-r(x_i)}\right], 
    \]
    which applies large penalty if $y_i = 1$ and $r(x_i)$ is close to
    zero or if $y_i = 0$ and $r(x_i)$ is close to 1.

    \bluebox $L$ is convex, so it can be reliably minimized using
    numerical optimization algorithms. 
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Support vector machines}, colframe
    = MidnightBlue!75!white, before upper =
    {\setcounter{boxcounter}{0}\setlength{\abovedisplayskip}{2pt}%
      \setlength{\belowdisplayskip}{2pt}}, colback = softblue, boxsep = 0.5pt,
    left = 2mm, right = 2mm, enhanced, breakable, parbox = false, drop
    fuzzy shadow]

    \begin{insetfigure}{\includegraphics[width=0.45\textwidth]{figures/svm}}[][5pt]
      \bluebox A \textbf{support vector machine} (SVM) chooses a
      hyperplane $H\subset \R^p$ and predicts classification
      ($\mathcal{Y} = \{-1,+1\}$) based on which side of $H$ the
      feature vector $\mathbf{x}$ lies on.

      \bluebox
      $\mathbf{x} \mapsto \operatorname{sgn}(\bdbeta\cdot \mathbf{x} -
      \alpha)$ is the prediction function, where
      \[
        H = \{\mathbf{x} \in \R^p \,:\, \bdbeta\cdot \mathbf{x} -
        \alpha = 0\}.
      \] 
     
      \bluebox We train the SVM with the risk 
    \end{insetfigure}
      \begin{equation*}
        L(\bdbeta,\alpha) = \lambda |\bdbeta|^2
        +  \frac{1}{n}\sum_{i=1}^{n}\big[1-y_i(\bdbeta\cdot \mathbf{x}_i
        - \alpha)\big]_+
      \end{equation*}

    where $[u]_+$ denotes $\max(0,u)$, the positive part of $u$. 

    \bluebox The parameters $\bdbeta$ and $\alpha$ encode both $H$ and
    the a distance---called the \textbf{margin}---from $H$ to a parallel
    hyperplane where we begin penalizing for lack of decisively
    correct classification. The margin is $1/|\bdbeta|$ (and can be
    adjusted without changing $H$ by scaling $\bdbeta$ and $\alpha$). 

    \begin{center}
      \includegraphics[width=0.88\textwidth]{figures/svm-margin}
    \end{center}

    \bluebox If $\lambda$ is small, then the optimization prioritizes
    the correctness term and uses a small margin if necessary. If
    $\lambda$ is large, the optimization must minimize a large-margin
    incorrectness penalty. A value for $\lambda$ may be chosen by
    cross-validation.

    \bluebox Applying a function $\phi$ to map the feature vectors to a higher dimensional allows us to find nonlinear separating surfaces in the original feature space. The function $K = (\mathbf{x}, \mathbf{y})\mapsto \phi(\mathbf{x})\cdot \phi(\mathbf{y})$ is called the \textit{kernel} associated with the transformation $\phi$. Finding the separating hyperplane in the higher dimensional space comes down to finding a vector $\bdalpha$ solving the problem (with $\odot$, $\oplus$, and $\preccurlyeq$ as pointwise multiplication, addition, and comparison)
    \begin{align*}
    &\text{minimize} \quad \frac{1}{2}(\bdalpha \odot \mathbf{y})'\mathcal{K}
    (\bdalpha \odot \mathbf{y}) - \operatorname{sum}(\bdalpha) \\
  &\text{subject to} \quad 0 \preccurlyeq \bdalpha \preccurlyeq C \text{ and }
    \bdalpha \cdot \mathbf{y} = 0.
    \end{align*}
    The prediction vector for an $n_{\mathrm{test}} \times n$ feature matrix $X_{\mathrm{test}}$ is 
      \[
        \mathcal{K}_{\mathrm{test}}(\widehat{\bdalpha}
        \odot \mathbf{y}) \oplus \widehat{b},
      \]     
    where $\mathcal{K}_{\mathrm{test}}$ is the
    $n_{\mathrm{test}} \times n$ matrix whose $(i,j)$th entry is obtained by applying $K$ to the $i$th row of $X_{\mathrm{test}}$ and the $j$th row of $X$, and where $\widehat{b}$ is any entry of
    \[
      \mathbf{y} - \mathcal{K}(\widehat{\bdalpha} \odot \mathbf{y})
    \]
    for which the corresponding entry in $\widehat{\bdalpha}$ is strictly between 0 and $C$. 

      \bluebox The prediction function (before taking the sign) is a constant plus a linear combination of functions of the form 

      \vspace{-1mm}
      
      \begin{insetfigure}{\includegraphics[width=0.4\textwidth]{figures/svm-humps-blue.png}}[][2pt] 
        $\mathbf{x}\mapsto K(\mathbf{x},\mathbf{x}_i)$, where $\mathbf{x}_i$ is the $i$th training observation. Example shown for 2D feature space with \textbf{radial basis function} kernel $K(\mathbf{x}_i,\mathbf{x}_j) = \operatorname{e}^{-|\mathbf{x}_i-\mathbf{x}_j|^2}$.
    \end{insetfigure}
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Neural networks}, colframe
    = MidnightBlue!75!white, before upper =
    {\setcounter{boxcounter}{0}\setlength{\abovedisplayskip}{2pt}%
      \setlength{\belowdisplayskip}{2pt}}, colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm, enhanced, breakable, parbox = false, drop fuzzy shadow]

    \bluebox A \textbf{multilayer perceptron} $N: \R^p \to \R^q$ is
    a composition of affine transformations and componentwise
    applications of a function $K: \R \to \R$.
    \begin{enumerate}[(i),leftmargin=20pt,itemsep=0pt,topsep=0pt]
    \item We call $K$ the \textbf{activation function}. Common
      choices: 
      \begin{enumerate}[(a),topsep=-2pt,itemsep=0pt,leftmargin=12pt]
      \item $u\mapsto \max(0,u)$ (rectifier, or ReLU) 
      \item $u\mapsto 1/(1+e^{-u})$ (logistic) 
      \end{enumerate}
    \item \textit{Component-wise application of $K$} on $\R^t$ refers
      to the function
      $K.(x_1, \ldots, x_t) = (K(x_1), \ldots, K(x_t))$. 
    \item An affine transformation from $\R^t$ to $\R^s$ is a map of
      the form $A(\mathbf{x}) = W\mathbf{x} + \mathbf{b}$, where $W$
      is an $s\times t$ matrix and $\mathbf{b} \in \R^s$. Entries of
      $W$ are called \textbf{weights} and entries of $\mathbf{b}$ are
      called \textbf{biases}. 
    \end{enumerate}

    \begin{center}
      \includegraphics[width=0.98\textwidth]{figures/nn}
    \end{center}

    \bluebox The \textbf{architecture} of a neural network is the
    sequence of dimensions of the domains and codomains of its affine
    maps. For example, a neural net with $W_1 \in \R^{5 \times 3}$,
    $W_2 \in \R^{4 \times 5}$, and $W_3 \in \R^{1 \times 4}$ has
    architecture $[3,5,4,1]$.

    \bluebox Given training samples $\{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N$,
    we obtain a neural net regression function by minimizing 
    $L(N) =
    \displaystyle{\sum_{i=1}^{n}C(N(\mathbf{x}_i),\mathbf{y}_i)}$
    where
    $C(\mathbf{y},\mathbf{y}_i) = |\mathbf{y} - \mathbf{y}_i|^2$.

    \bluebox For classification, we
    \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
    \item let
      $\mathbf{y}_i = [0,\ldots,0,1,0,\ldots 0] \in
      \R^{|\mathcal{Y}|}$, with the location of the nonzero entry
      indicating class (this is called \textbf{one-hot encoding}), 
    \item replace the identity map in the diagram with
    the \textbf{softmax} function
    $\mathbf{u} \mapsto \left[\e^{u_j}/\left(\sum_{k=1}^n
        \e^{u_k}\right)\right]_{j=1}^{|\mathcal{Y}|}$, and
    \item replace
    the cost function with
    $C(\mathbf{y}, \mathbf{y}_i) = - \log(\mathbf{y} \cdot
    \mathbf{y}_i)$.
  \end{enumerate}
  
    \bluebox When the weight matrices are large, they have many
    parameters to tune. We use a custom optimization scheme:

    \begin{enumerate}[(i),topsep=0pt,itemsep=1pt,labelsep=2pt]
    \item Start with random weights and a training input
      $\mathbf{x}_i$. 
    \item \textbf{Forward propagation}: apply each successive map and
      store the vectors at each green or purple node. The vectors
      stored at the green nodes are called \textbf{activations}. 
    \item \textbf{Backpropagation}: starting with the \textit{last}
      green node and working left, compute the change in cost per
      small change in the vector at each green or purple node. By the
      chain rule, each such gradient is equal to the gradient computed
      at right-adjacent node times the derivative of map between the
      two nodes. The derivative of $A_j$ is $W_j$, and the derivative
      of $K.$ is
      $\mathbf{v}\mapsto \operatorname{diag}\left(\left(\tfrac{\d
            K}{\d u}\right)_.(\mathbf{v})\right)$.
    \item Compute the change in cost per small change in the weights
      and biases at each blue node. Each such gradient is equal to the
      gradient stored at the next purple node times the derivative of
      the intervening affine map. We have
      $\frac{\partial(W\mathbf{x} + \mathbf{b})}{\partial \mathbf{b}} =
      I$ and
      $\mathbf{v}\frac{\partial(W\mathbf{x} + \mathbf{b})}{\partial W} =
      \mathbf{v}'\mathbf{x}'$.

    \item \textbf{Stochastic gradient descent}: repeat (ii)--(iv) for
      each sample in a \textit{randomly chosen subset} of the training
      set and determine the average desired change in weights and
      biases to reduce the cost function. Update the weights and
      biases accordingly and iterate to convergence. 
    \end{enumerate}
  \end{tcolorbox}

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
      Statistical Learning: Dimension reduction}, colframe
    = MidnightBlue!75!white, before upper =
    {\setcounter{boxcounter}{0}\setlength{\abovedisplayskip}{2pt}%
      \setlength{\belowdisplayskip}{2pt}}, colback = softblue, boxsep = 0.5pt,
    left = 2mm, right = 2mm, enhanced, breakable, parbox = false, drop
    fuzzy shadow, break at=395pt/0pt]

    \bluebox The goal of \textbf{dimension reduction} is to map a set
    of $n$ points in $\R^p$ to a lower-dimensional space $\R^k$ while
    retaining as much of the data's structure as possible.

    \bluebox Dimension reduction can be used as a visualization aid or
    as a feature pre-processing step in a machine learning model. 

    \bluebox \textit{Structure} may be taken to mean \textit{variation
      about the center}, in which case we use \textbf{principal
      component analysis}:
    \begin{enumerate}[(i),topsep=0pt,itemsep=1pt,labelsep=2pt]
    \item Store the points' components in an $n \times p$ matrix,
    \item de-mean each column,
    \item compute the SVD $U \Sigma V'$ of the resulting matrix, and 
    \item let $W$ be the first $k$ columns of $V$.
    \end{enumerate}
    Then $WW\transpose: \R^p \to \R^p$ is the rank-$k$ projection
    matrix which minimizes the sum of squared projection distances of
    the points, and $W\transpose : \R^p \to \R^k$ maps each point to
    its coordinates in that $k$-dimensional subspace (with respect to
    the columns of $W$). 

    \begin{center}
      \includegraphics[width=\textwidth]{figures/mnist-pca-blue}
    \end{center}
    
    \bluebox \textit{Structure} may be taken to mean \textit{pairwise
      proximity of points}, which \textbf{stochastic neighbor
      embedding} attempts to preserve.  Given the data points
    $\mathbf{x}_1, \ldots, \mathbf{x}_n$ and a parameter $\rho$ called
    the \textbf{perplexity} of the model, we define
    \[
      P_{i,j}(\sigma) = \frac{\e^{-|\mathbf{x}_i -
          \mathbf{x}_j|^2/(2\sigma^2)}}{\sum_{k\neq j} \e^{-|\mathbf{x}_k
          - \mathbf{x}_j|^2/(2\sigma^2)}}, 
    \]
    and for each $j$ we define $\sigma_j$ to be the solution $\sigma$ of
    the equation
    \[
      2^{-\sum_{i \neq j} P_{i,j}(\sigma) \log_2 P_{i,j}(\sigma)} =
      \rho.
    \]
    We define 
    $p_{i,j} = \frac{1}{2n}(P_{i,j}(\sigma_j) + P_{j,i}(\sigma_i))$, 
    which describes the similarity of $\mathbf{x}_i$ and
    $\mathbf{x}_j$. Given $\mathbf{y}_1, \ldots, \mathbf{y}_n$ in
    $\R^k$, we define similarities 
     \[
       q_{i,j} = \frac{(1 +
         |\mathbf{y}_i-\mathbf{y}_j|^2)^{-1}}{\sum_{k\neq j}(1 +
         |\mathbf{y}_k-\mathbf{y}_j|^2)^{-1}}. 
     \]
     We choose $\mathbf{y}_1, \ldots, \mathbf{y}_n$ to minimize 
     \[
       C(\mathbf{y}_1, \ldots, \mathbf{y}_n) = \sum_{1 \leq i \neq j
         \leq n} p_{i,j} \log_2 \frac{p_{i,j}}{q_{i,j}}.
     \]

     \begin{center}
       \includegraphics[width=0.9\textwidth]{figures/mnist-tsne}
     \end{center}

   \end{tcolorbox}

   \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistical Learning: Bayesian inference}, colframe =
  MidnightBlue!75!white, before upper = \setcounter{boxcounter}{0},
  colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm,
  enhanced, breakable, parbox = false, drop fuzzy shadow]

  \bluebox In contrast to frequentist statistics, which represents model parameters as fixed and unknown, Bayesian statistics regards model parameters as random variables with a specified \textbf{prior} distribution. Observed data are used to obtain an updated \textbf{posterior} distribution, via Bayes' theorem.
  \textit{Example:}

  \vspace{-1.1mm}

  \begin{insetfigure}{\includegraphics[width=0.45\textwidth]{figures/beta-posterior}}[][6pt]
  \textit{the prior for the heads probability of a weighted coin might be stipulated to be uniform on $[0,1]$. Then observing $p$ heads and $q$ tails results in a posterior distribution proportional to $t\mapsto t^{p}(1-t)^{q}$. Frequentist statistics would instead give a single point estimate (such as the maximum likelihood estimator $p / (p + q)$).}
  \end{insetfigure}

  \bluebox \textbf{Posterior is proportional to likelihood times prior}: if $X$ is the observed random variable and $\Theta$ the vector of model parameters, then
  \begin{align*}
    \overbrace{f(\theta \given x)}^{\text{posterior}} = \frac{\overbrace{f(x \given \theta)}^{\text{likelihood}}\overbrace{f(\theta)}^{\text{prior}}}{\underbrace{f(x)}_{\text{marginal}}}
  \end{align*}

  where $f(\theta \given x)$ is shorthand for the conditional density or mass function of $\Theta$ given $X = x$. 
  
  \bluebox If the prior and posterior distributions belong to the same family of distributions, they are called \textbf{conjugate}. \textit{Example: heads-probability distributions of the form $t\mapsto t^{p}(1-t)^{q}$ on $[0,1]$ update under coin flip observations by incrementing the exponents $p$ and $q$, so they are conjugate priors for the coin flip problem.}

  \bluebox Posterior distributions yield point estimates via measures of central tendency like the median or mean, as well as \textbf{Bayesian posterior intervals} (similar to confidence intervals) via their quantiles. 

  \bluebox Bayesian and frequentist statistics often yield similar results in the limit: under quite general conditions, the posterior distribution is approximately normal with mean converging to the maximum likelihood estimator as the sample size goes to $\infty$.

  \bluebox Bayesian analysis often involves evaluating integrals. For example, the posterior mean is $\frac{\int_{\mathbb{R}^n} \theta \mathcal{L}(\theta) f(\theta)\, \d\theta }{\int_{\mathbb{R}^n} \mathcal{L}(\theta) f(\theta)\, \d\theta }$, where $\mathcal{L}(\theta) = f(x\given \theta)$ is the likelihood. These integrals are often impossible to solve analytically or even approximate using exact numerical methods in the case where the parameter space is high-dimensional. One solution is to use \textbf{Monte Carlo} methods: use the identity $\int_{\mathbb{R}^n} g(x) \, f(x) \, \d x = \E[g(X)]$ where $X$ is a random vector with density $f$. The expectation can be approximated by sampling repeatedly from the density $f$, using the law of large numbers.

  \bluebox \textbf{Markov Chain Monte Carlo} (MCMC) is useful for approximating $\int_{\mathbb{R}^n} g(x) \, f(x) \, \d x$ when sampling from $f$ is difficult. Metropolis-Hastings is a common class of MCMC algorithms:
  \begin{enumerate}[itemsep=0pt,topsep=0pt,labelsep = 2pt]
    \item For each $x\in \mathbb{R}^n$, let $q(x)$ be a distribution on ${\mathbb{R}^n}$ that we can readily sample from, and for which $q(x)(y) = q(y)(x)$.
    \item Choose $X_0$ arbitrarily, and sample $X_{\text{new}}$ from the distribution $q(X_0)$
    \item Define $X_1$ to be $X_{\text{new}}$ with probability $\frac{f(X_{\text{new}})}{f(X_0)}$ (or 1, if the given ratio exceeds 1) and $X_0$ otherwise.
    \item Repeat steps (ii) and (iii) to obtain $X_2$ from $X_1$, $X_3$ from $X_2$, and so on.
  \end{enumerate}
  The resulting sequence $X_0, X_1, \ldots$ has the property that the distribution of $X_n$ converges to $f$ as $n\to\infty$, as well as the property that the mean of the list $[g(X_0),g(X_1),\ldots,g(X_n)]$ converges to $\int_{\mathbb{R}^n} g(x) \, f(x) \, \d x$. 

  \bluebox Popular Metropolis algorithms:

  \begin{itemize}[itemsep=0pt,topsep=0pt,labelsep = 2pt,leftmargin=6pt]
    \item \textbf{Hamiltonian Monte Carlo} (HMC). Suitable for continuous variables and much faster than plain Metropolis-Hastings with a Gaussian proposal distribution. Requires the ability to differentiate the density with respect to the variables (often handled using autodiff).
    \item \textbf{No U-Turn Sampler} (NUTS). A common variant of HMC. 
    \item \textbf{Particle Gibbs} (PG). Suitable for discrete variables. 
    \item \textbf{Gibbs Sampler}. Gibbs sampling allows us to modify different variables using different samplers: we alternatingly hold one set of variables fixed while proposing a Metropolis update to the others, then hold the latter set fixed while proposing an update to the former set.
  \end{itemize}

  \bluebox One disadvantage of Bayesian statistics is the subjectivity of the prior distribution. On the other hand, when a meaningful prior is available, Bayesian statistics provides a natural way to combine that information with the observed data. Frequentist and Bayesian statistics both have strengths and weaknesses which can vary in importance depending on the details of the problem at hand.

  \end{tcolorbox} 

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
  Statistical Learning: Graphical models}, colframe =
  MidnightBlue!75!white, before upper = \setcounter{boxcounter}{0},
  colback = softblue, boxsep = 0.5pt, left = 2mm, right = 2mm,
  enhanced, breakable, parbox = false, drop fuzzy shadow]

  \bluebox A \textbf{Bayes net} is a random vector together with a directed

  \vspace{-1.1mm}

    \begin{insetfigure}{\includegraphics[width=0.3\textwidth]{figures/bayes-net}}[][8pt]
      acyclic graph (DAG) which models conditional dependence relationships among its components.

    The random vector $\mathbf{X} = (X_1, \ldots, X_5)$ and the graph shown make a Bayes net if the distribution of $\mathbf{X}$ factors as a product of conditional distributions as indicated by the graph connections; that is, if for all $(x_1, \ldots, x_5)$, we have
    \end{insetfigure}

    \vspace{-2mm}

    \begin{align*}\mathbb{P}(\mathbf{X} = (x_1, x_2, \ldots, x_5)) = \, &\mathbb{P}(X_1 = x_1) \times \\ &\mathbb{P}(X_2=x_2\given X_1 = x_1)\times \\ &\mathbb{P}(X_3=x_3\given X_1 = x_1)\times \\ &\mathbb{P}(X_4=x_4\given X_2 = x_2,X_3 = x_3)
      \times \\ &\mathbb{P}(X_5=x_5\given X_3 = x_3)
    \end{align*}

    \bluebox In many Bayes net applications, only some of the random variables are observed. The others are called \textit{hidden} or \textit{latent} variables. This missing information presents 
    inference challenges.

    \begin{insetfigure}{\includegraphics[width=0.1\textwidth]{figures/gmm}}[][6pt]
    \bluebox A \textbf{Gaussian mixture model} (GMM) is a Bayes net consisting of a discrete random variable $Z$ and a random variable $X$ whose conditional distribution given each possible value of $Z$ is Gaussian.
    \end{insetfigure}

    \bluebox A \textbf{hidden Markov model} is a Bayes net consisting of a chain of random variables $Z_1, \ldots, Z_n$ (called \textit{hidden} variables), each of which is the parent of a single random variable $X_i$: 

    \vspace{-2.5mm}
    
    \begin{center}
      \includegraphics[width=0.55\textwidth]{figures/hidden-markov}
    \end{center}

    \vspace{-2mm}

    \bluebox Bayes net inference (drawing conclusions about the model based on observed data) can be carried out using a maximum likelihood technique called expectation-maximization (EM) or using Bayesian MCMC methods. 

    \bluebox \textbf{Expectation-Maximization} is an iterative procedure for parameter estimation in models with hidden variables: start with a random guess for the parameters and find the conditional distribution $\zeta$ of the hidden variables given the observed variables and the current parameter guess. We then treat the parameter vector $\theta$ as unknown and compute---with respect to the measure $\zeta$---the expected log likelihood function $Q(\theta)$. New parameters are chosen to maximize $Q$, and the two steps are iterated to convergence. 

    \textit{Example. Consider a GMM with a $\{0,1\}$-valued $Z$: we have $\P(Z = 1) = \alpha$, and for each observation $i$ and each $j \in \{0,1\}$, the conditional distribution of $\mathbf{X}_i$ given $Z_i = j$ is normal with mean $\bdmu_j$ and covariance $\Sigma_j$. All together, the parameter vector is $\theta = (\alpha, \bdmu_0, \Sigma_0, \bdmu_1, \Sigma_1)$. By Bayes' Theorem, the conditional distribution of $Z_i$ given $\mathbf{X}_i = \mathbf{x}_i$ is Bernoulli with success probability}
    \[
      \pi_i = \frac{\alpha f_1(\mathbf{x}_i)}{\alpha f_1(\mathbf{x}_i) + (1-\alpha) f_0(\mathbf{x}_i)}, 
    \]
      \textit{where $f_j$ is the normal density with mean $\bdmu_j$ and covariance $\Sigma_j$. Then, treating the $\pi_i$'s as constant, we get
    \begin{align*}
      Q(\theta) &= \E\left[\log \prod_{i=1}^n(z_i\alpha f_1(\mathbf{x}_j)+(1-z_i)(1-\alpha) f_0(\mathbf{x}_j))\right] \\
      &= \sum_{i=1}^n \pi_i[\log \alpha + \log f_1(\mathbf{x}_i)] \\ &\hspace{6mm} + 
      (1-\pi_i)[\log (1-\alpha) + \log f_1(\mathbf{x}_i)].
    \end{align*}
    Optimizing, we get $\pi$-weighted counts, means, and covariance matrices: $\alpha = \frac{1}{n}\sum_{i=1}^n \pi_i$ and 
    \begin{align*}
      \bdmu_1 &= \sum_{i=1}^n \pi_i \mathbf{x}_i\bigg/\sum_{i=1}^n \pi_i, \quad \bdmu_0 = \sum_{i=1}^n (1-\pi_i) \mathbf{x}_i\bigg/\sum_{i=1}^n (1-\pi_i) \\
      \Sigma_1 &= \sum_{i=1}^n \pi_i (\mathbf{x}_i - \bdmu_1)(\mathbf{x}_i - \bdmu_1)\transpose\bigg/\sum_{i=1}^n \pi_i, \\
      \Sigma_0 &= \sum_{i=1}^n (1-\pi_i) (\mathbf{x}_i - \bdmu_1)(\mathbf{x}_i - \bdmu_1)\transpose\bigg/\sum_{i=1}^n (1-\pi_i) 
    \end{align*}
    In the EM iterations shown, membership probabilities $\pi_i$, based on current parameter estimates, are indicated by point color (E-step). These values are used as weights to update the means and covariances for the multivariate normal distributions (M-step). 
    }
    \begin{center}
      \includegraphics[width=0.32\textwidth]{figures/gmm-step-0}
      \includegraphics[width=0.32\textwidth]{figures/gmm-step-1}
      \includegraphics[width=0.32\textwidth]{figures/gmm-step-2}
    \end{center}

  \bluebox A \textbf{Probabilistic Programming Language} (PPL) is a framework for describing stochastic models and performing inference on them. Examples: \textbf{Stan} (a C++ library, callable from Julia/Python/R), \textbf{PyMC3}, and \textbf{Turing.jl}. 

  \bluebox A HMM example in Turing.jl (the object returned on the last line will contain estimates for the parameters):

  \begin{julblock}[colback = softblue, colframe=softblue, left = 6pt]
    using Turing
    @model HMM(x) = begin
        n = length(x)
        z = tzeros(Int64, n) # hidden states
        p₁ ~ Uniform(0,1) # trans. prob. 1→1
        p₂ ~ Uniform(0,1) # trans. prob. 2→1
        P = [p₁ 1-p₁ # transition matrix
             p₂ 1-p₂] 
        # starting state is 1 or 2, equal prob.
        z[1] ~ Categorical([0.5,0.5]) 
        x[1] ~ Normal(z[1],0.1)
        for i=2:n
            # choose next hidden state based
            # on row of transition matrix P
            z[i] ~ Categorical(P[z[i-1],:])
            # add noise to get x
            x[i] ~ Normal(z[i],0.1)
        end
    end

    # choose parameters for samplers
    hmc = HMC(2, 0.001, 7, :p₁, :p₂)
    pg = PG(20, 1, :z)
    G = Gibbs(1000, hmc, pg)

    # perform inference (assuming the vector x 
    # contains empirical observations)
    sample(HMM(x), G)
  \end{julblock}


  \end{tcolorbox} 

  \begin{tcolorbox}[title={\fontspec{American Typewriter Bold}
    Statistics: dplyr and ggplot2}, colframe = yellow!40!black, breakable,
  left = 2mm, right = 2mm, colback = softyellow, boxsep = 0.5pt,
  before upper = \setcounter{boxcounter}{0}, enhanced, parbox =
  false, drop fuzzy shadow]

  \ybox \rverb{dplyr} is an R package for manipulating data
  frames. The following functions filter rows, sort rows, select
  columns, add columns, group, and aggregate the columns of a
  grouped data frame. 
  \begin{Rblock}[left=2pt]
    flights %>% 
      filter(month == 1, day < 5) %>% 
      arrange(day, distance) %>% 
      select(month, day, distance, air_time) %>% 
      mutate(speed = distance / air_time * 60) %>% 
      group_by(day) %>% 
      summarise(avgspeed = mean(speed,na.rm=TRUE)) 
  \end{Rblock}

  \ybox \rverb{ggplot2} is an R package for data
  visualization. Graphics are built as a sum of \textit{layers}, which
  consist of a data frame, a \textbf{geom}, a \textbf{stat}, and a
  mapping from the data to the geom's \textbf{aesthetics} (like
  \rverb{x}, \rverb{y}, \rverb{color}, or \rverb{size}). The
  appearance of the plot can be customized with \jlverb{scale}s, \rverb{coord}s, and
  \rverb{theme}s. 

  \begin{Rblock}[left=2pt]
  ggplot(data = mpg) + 
      geom_smooth(mapping = aes(x = displ, y = hwy)) + 
      geom_point(mapping = aes(x = displ, y = hwy, 
                           color = class, alpha = cty))
  \end{Rblock}

  \vspace{-1mm}

  \begin{center}
    \includegraphics[width=0.93\textwidth]{figures/ggplot-example}
  \end{center}

\end{tcolorbox}

\end{multicols*}

\end{document}
